{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## First commint to GitHub"
      ],
      "metadata": {
        "id": "eefkLYk01aJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf25uYCu8Orn",
        "outputId": "0f560279-a96c-4753-b35e-e2e4f1a19d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  DATA CLEANING & PREPROCESSING\n",
        "\n"
      ],
      "metadata": {
        "id": "AIYfbYQlprBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up & Imports"
      ],
      "metadata": {
        "id": "mqzqKc5TCHuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict\n",
        "import sys, json, glob, re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Versions -> pandas:\", pd.__version__, \"| numpy:\", np.__version__)"
      ],
      "metadata": {
        "id": "Jd4UXRmzpwuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config"
      ],
      "metadata": {
        "id": "aVaC2byzCl48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_DRIVE = True                            # Mount Google Drive to search for the file\n",
        "DRIVE_SEARCH_DIR = \"/content/drive/MyDrive\" # Root folder to search\n",
        "FILE_PATTERN = \"**/Phishing_Mendeley*.csv\"  # Pattern to locate your CSV in Drive\n",
        "FALLBACK_PROMPT_UPLOAD = True               # If not found, open an upload dialog\n",
        "\n",
        "# Behaviors\n",
        "DROP_DUPLICATES = True                      # Only drop full-row duplicates (incl. 'id'); if id differs -> keep\n",
        "DROP_HIGH_MISSING_COLS = False              # If True, drop cols with missing rate > HIGH_MISSING_THRESHOLD\n",
        "HIGH_MISSING_THRESHOLD = 0.40\n",
        "\n",
        "# Save to Drive too?\n",
        "SAVE_BACK_TO_DRIVE = False                  # If True, also copy outputs to DRIVE_OUT_DIR\n",
        "DRIVE_OUT_DIR = \"/content/drive/MyDrive/phishing_cleaned_outputs\""
      ],
      "metadata": {
        "id": "x5hF-nyiCmVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive/Load Utilities"
      ],
      "metadata": {
        "id": "-rKW50bkCn2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive_if_needed():\n",
        "    if USE_DRIVE:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Drive mounted.\")\n",
        "        except Exception as e:\n",
        "            print(\"Drive mount failed or not in Colab:\", e)\n",
        "\n",
        "def find_csv_in_drive(search_dir: str, pattern: str) -> Optional[str]:\n",
        "    paths = glob.glob(str(Path(search_dir) / pattern), recursive=True)\n",
        "    return max(paths, key=lambda p: Path(p).stat().st_mtime) if paths else None\n",
        "\n",
        "def upload_dialog() -> Optional[str]:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Please upload your CSV fileâ€¦\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            return None\n",
        "        name = next(iter(uploaded.keys()))\n",
        "        print(\"Uploaded:\", name)\n",
        "        return str(Path(\"/content\") / name)\n",
        "    except Exception as e:\n",
        "        print(\"Upload dialog not available (not in Colab?):\", e)\n",
        "        return None\n",
        "\n",
        "def read_raw(p: str | Path) -> pd.DataFrame:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return pd.read_csv(p, encoding=enc, engine=\"python\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise RuntimeError(\"Failed to read CSV with utf-8 / utf-8-sig / latin-1.\")"
      ],
      "metadata": {
        "id": "9FfiynjkCq_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Locate & Load the CSV"
      ],
      "metadata": {
        "id": "CRti-H5YCtFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mount_drive_if_needed()\n",
        "\n",
        "csv_path = None\n",
        "if USE_DRIVE:\n",
        "    csv_path = find_csv_in_drive(DRIVE_SEARCH_DIR, FILE_PATTERN)\n",
        "    print(\"Drive search:\", \"FOUND\" if csv_path else \"Not found\")\n",
        "\n",
        "if not csv_path and FALLBACK_PROMPT_UPLOAD:\n",
        "    csv_path = upload_dialog()\n",
        "\n",
        "if not csv_path:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not locate a dataset. Set USE_DRIVE=True with correct DRIVE_SEARCH_DIR/FILE_PATTERN \"\n",
        "        \"or enable FALLBACK_PROMPT_UPLOAD.\"\n",
        "    )\n",
        "\n",
        "print(\"Using dataset:\", csv_path)\n",
        "\n",
        "raw_df = read_raw(csv_path)\n",
        "orig_shape = (int(raw_df.shape[0]), int(raw_df.shape[1]))\n",
        "\n",
        "# Preserve CamelCase names; trim whitespace inside string cells\n",
        "df = raw_df.copy()\n",
        "for c in df.select_dtypes(include=[object]).columns:\n",
        "    df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "print(\"Loaded shape:\", df.shape)"
      ],
      "metadata": {
        "id": "ntWEv5kaCtu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info"
      ],
      "metadata": {
        "id": "axdgtcpOCu2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from textwrap import indent\n",
        "\n",
        "print(\"\\n=== DATASET INFO ===\")\n",
        "print(\"Path:\", csv_path)\n",
        "print(\"Original shape:\", orig_shape)\n",
        "\n",
        "print(\"\\n.dtypes (first 50):\")\n",
        "print(df.dtypes.head(50))\n",
        "\n",
        "print(\"\\n.info():\")\n",
        "# Capture df.info() into a string buffer so we can print it nicely\n",
        "buffer = io.StringIO()\n",
        "df.info(buf=buffer)\n",
        "info_str = buffer.getvalue()\n",
        "print(info_str)\n",
        "\n",
        "print(\"\\n.head(5):\")\n",
        "display(df.head(5))\n",
        "\n",
        "print(\"\\nMissingness (top 20):\")\n",
        "miss = df.isna().mean().sort_values(ascending=False)\n",
        "display(miss.head(20).to_frame(\"missing_rate\"))\n",
        "\n",
        "# Early guess of target column (just for info; final alignment happens later)\n",
        "target_guess = next(\n",
        "    (c for c in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]\n",
        "     if c in df.columns),\n",
        "    None\n",
        ")\n",
        "print(\"\\nTarget column guess:\", target_guess)\n",
        "if target_guess is not None:\n",
        "    # Show a small sample of unique values\n",
        "    try:\n",
        "        uniques = pd.unique(df[target_guess].dropna())\n",
        "        print(\"Sample target values:\", uniques[:10])\n",
        "    except Exception as e:\n",
        "        print(\"Could not preview target values:\", e)"
      ],
      "metadata": {
        "id": "Z6WPc6GSCx7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions for Cleaning"
      ],
      "metadata": {
        "id": "rZbpPFIXCzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coerce_numeric_like(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Coerce object columns that look numeric (>=80% numeric-like) into numeric dtype.\"\"\"\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            s = df[c].astype(str).str.strip()\n",
        "            mask = s.str.match(r'^[+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?$')\n",
        "            if mask.mean() >= 0.8:\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def handle_infinities(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def drop_low_variance(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, list]:\n",
        "    nunique = df.nunique(dropna=False)\n",
        "    lowvar = nunique[nunique <= 1].index.tolist()\n",
        "    if target_col in lowvar:\n",
        "        lowvar.remove(target_col)\n",
        "    if lowvar:\n",
        "        df = df.drop(columns=lowvar)\n",
        "    return df, lowvar\n",
        "\n",
        "def impute_missing(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, Dict]:\n",
        "    df = df.copy()\n",
        "    report: Dict = {}\n",
        "\n",
        "    if DROP_HIGH_MISSING_COLS:\n",
        "        miss_rate = df.isna().mean().sort_values(ascending=False)\n",
        "        drop_cols = miss_rate[miss_rate > HIGH_MISSING_THRESHOLD].index.tolist()\n",
        "        if target_col in drop_cols:\n",
        "            drop_cols.remove(target_col)\n",
        "        if drop_cols:\n",
        "            df = df.drop(columns=drop_cols)\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": drop_cols\n",
        "        }\n",
        "    else:\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": []\n",
        "        }\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
        "    if target_col and target_col in cat_cols:\n",
        "        cat_cols.remove(target_col)\n",
        "\n",
        "    imputations = {\"numeric\": {}, \"categorical\": {}}\n",
        "    for c in num_cols:\n",
        "        if df[c].isna().any():\n",
        "            med = df[c].median()\n",
        "            df[c] = df[c].fillna(med)\n",
        "            imputations[\"numeric\"][c] = None if pd.isna(med) else float(med)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if df[c].isna().any():\n",
        "            mode = df[c].mode(dropna=True)\n",
        "            val = mode.iloc[0] if not mode.empty else \"__missing__\"\n",
        "            df[c] = df[c].fillna(val)\n",
        "            imputations[\"categorical\"][c] = val\n",
        "\n",
        "    report[\"imputations\"] = imputations\n",
        "    return df, report\n",
        "\n",
        "def finalize_int_casts(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_float_dtype(df[c]):\n",
        "            s = df[c]\n",
        "            if np.allclose(s.dropna() % 1, 0):\n",
        "                try:\n",
        "                    df[c] = s.astype(\"Int64\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return df\n",
        "\n",
        "def make_jsonable(obj):\n",
        "    import numpy as _np\n",
        "    import pandas as _pd\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_jsonable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple, set)):\n",
        "        return [make_jsonable(v) for v in obj]\n",
        "    if isinstance(obj, (_np.integer,)):\n",
        "        return int(obj)\n",
        "    if isinstance(obj, (_np.floating,)):\n",
        "        return float(obj)\n",
        "    if isinstance(obj, (_np.bool_,)):\n",
        "        return bool(obj)\n",
        "    if isinstance(obj, _np.ndarray):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.Series):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.DataFrame):\n",
        "        return obj.to_dict(orient=\"list\")\n",
        "    return obj"
      ],
      "metadata": {
        "id": "4AlaijahC1XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Pipeline"
      ],
      "metadata": {
        "id": "aO2jMAy3C2tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_df = df.copy()\n",
        "report = {\n",
        "    \"source\": csv_path,\n",
        "    \"original_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"settings\": {\n",
        "        \"DROP_DUPLICATES\": bool(DROP_DUPLICATES),\n",
        "        \"DROP_HIGH_MISSING_COLS\": bool(DROP_HIGH_MISSING_COLS),\n",
        "        \"HIGH_MISSING_THRESHOLD\": float(HIGH_MISSING_THRESHOLD),\n",
        "    },\n",
        "    \"steps\": {}\n",
        "}\n",
        "\n",
        "# Duplicates (full-row)\n",
        "dup_count = int(work_df.duplicated().sum())\n",
        "if DROP_DUPLICATES and dup_count > 0:\n",
        "    work_df = work_df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
        "report[\"steps\"][\"duplicate_rows_found_full_row\"] = dup_count\n",
        "report[\"steps\"][\"duplicates_removed\"] = int(dup_count if DROP_DUPLICATES else 0)\n",
        "print(f\"Duplicates found: {dup_count} | Removed: {report['steps']['duplicates_removed']}\")\n",
        "\n",
        "# Drop non-predictive ID AFTER dedupe\n",
        "dropped_non_predictive = []\n",
        "if \"id\" in work_df.columns:\n",
        "    work_df = work_df.drop(columns=[\"id\"])\n",
        "    dropped_non_predictive.append(\"id\")\n",
        "report[\"steps\"][\"dropped_non_predictive\"] = dropped_non_predictive\n",
        "if dropped_non_predictive:\n",
        "    print(\"Dropped columns (non-predictive):\", dropped_non_predictive)\n",
        "\n",
        "# Coerce numeric-like; handle Â±inf\n",
        "work_df = coerce_numeric_like(work_df)\n",
        "work_df = handle_infinities(work_df)\n",
        "\n",
        "# Target alignment (prefer CLASS_LABEL, but auto-detect if changed)\n",
        "target_col = None\n",
        "for cand in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]:\n",
        "    if cand in work_df.columns:\n",
        "        target_col = cand\n",
        "        break\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found (expected 'CLASS_LABEL' or close variant).\")\n",
        "\n",
        "# Ensure numeric binary target\n",
        "if work_df[target_col].dtype == object:\n",
        "    y_num = pd.to_numeric(work_df[target_col], errors=\"coerce\")\n",
        "    if y_num.isna().any():\n",
        "        y_num = pd.Series(pd.factorize(work_df[target_col].astype(str).str.strip().str.lower())[0], index=work_df.index)\n",
        "    work_df[target_col] = y_num\n",
        "\n",
        "uniq = set(pd.unique(work_df[target_col].dropna()))\n",
        "if uniq.issubset({-1, 0, 1}) and uniq != {0, 1}:\n",
        "    # If dataset uses -1/1 or -1/0/1, map negatives to 1 (phishing) and non-negatives to 0\n",
        "    work_df[target_col] = work_df[target_col].map(lambda v: 1 if v < 0 else (0 if v > 0 else 0))\n",
        "\n",
        "report[\"steps\"][\"target_info\"] = {\n",
        "    \"name\": target_col,\n",
        "    \"unique_values_after_normalization\": sorted([int(x) for x in pd.unique(work_df[target_col].dropna())])\n",
        "}\n",
        "print(\"Target column:\", target_col)\n",
        "print(\"Target uniques (post-normalization):\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "\n",
        "# Drop truly constant columns (except target)\n",
        "work_df, lowvar_dropped = drop_low_variance(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"low_variance_dropped\"] = lowvar_dropped\n",
        "if lowvar_dropped:\n",
        "    print(\"Dropped low-variance cols:\", lowvar_dropped)\n",
        "\n",
        "# Impute missing values (no row dropping)\n",
        "work_df, mv_report = impute_missing(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"missing_value_handling\"] = mv_report\n",
        "print(\"Imputation summary:\", json.dumps(mv_report, indent=2)[:1000], \"...\")\n",
        "\n",
        "# Cast floats-that-are-integers to Int64\n",
        "work_df = finalize_int_casts(work_df)\n",
        "\n",
        "print(\"\\nPost-clean shape:\", work_df.shape)"
      ],
      "metadata": {
        "id": "CBXKaKKwC4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Cleaned Data & Report"
      ],
      "metadata": {
        "id": "_utgsuMEC5ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dir = Path(csv_path).parent  # same folder as original file\n",
        "csv_out = orig_dir / \"phishing_mendeley_cleaned.csv\"\n",
        "json_report = orig_dir / \"phishing_mendeley_cleaned_report.json\"\n",
        "\n",
        "# Save cleaned CSV\n",
        "work_df.to_csv(csv_out, index=False)\n",
        "\n",
        "# Update & save JSON report\n",
        "report.update({\n",
        "    \"final_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"row_delta\": int(work_df.shape[0] - orig_shape[0]),\n",
        "    \"outputs\": {\n",
        "        \"csv\": str(csv_out),\n",
        "        \"json_report\": str(json_report),\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(json_report, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump(make_jsonable(report), jf, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Saved CSV       :\", csv_out)\n",
        "print(\"Saved JSON      :\", json_report)\n",
        "\n",
        "# Optional copy back to Drive output folder\n",
        "if SAVE_BACK_TO_DRIVE and USE_DRIVE:\n",
        "    outdir = Path(DRIVE_OUT_DIR)\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    dst_csv = outdir / csv_out.name\n",
        "    dst_json = outdir / json_report.name\n",
        "    _ = Path(dst_csv).write_bytes(Path(csv_out).read_bytes())\n",
        "    _ = Path(dst_json).write_bytes(Path(json_report).read_bytes())\n",
        "    print(\"Also copied to  :\", outdir)"
      ],
      "metadata": {
        "id": "9JbkO6PPC7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Summary & Target Distribution"
      ],
      "metadata": {
        "id": "LaWqFpWIC-Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"Source          :\", report['source'])\n",
        "print(\"Original shape  :\", report['original_shape'])\n",
        "print(\"Final shape     :\", report['final_shape'])\n",
        "print(\"Row delta       :\", report['row_delta'])\n",
        "print(\"Dup (full-row)  :\", report['steps']['duplicate_rows_found_full_row'],\n",
        "      \"| removed:\", report['steps']['duplicates_removed'])\n",
        "print(\"Dropped (non-predictive):\", report['steps']['dropped_non_predictive'])\n",
        "\n",
        "print(\"\\nTarget:\", report[\"steps\"][\"target_info\"][\"name\"])\n",
        "print(\"Target uniques :\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "print(\"\\nTarget distribution:\")\n",
        "display(work_df[report[\"steps\"][\"target_info\"][\"name\"]].value_counts(dropna=False).to_frame(\"count\"))\n",
        "\n",
        "print(\"\\nPreview cleaned data:\")\n",
        "display(work_df.head(10))"
      ],
      "metadata": {
        "id": "Wzi7_2yfC90N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ MODEL TRAINING ============"
      ],
      "metadata": {
        "id": "2sJvFhP-8c_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHONG MUN SEONG (TP063440)"
      ],
      "metadata": {
        "id": "tS5HMHxv73VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOO CHEN KANG (TP065578)"
      ],
      "metadata": {
        "id": "meABj23S79Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENG YI LING (TP065686)"
      ],
      "metadata": {
        "id": "x_b5p5YN7-NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ FINAL RESULT (XGBOOST) ============"
      ],
      "metadata": {
        "id": "sXaTWech89Le"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Ife0CmG9Ff5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
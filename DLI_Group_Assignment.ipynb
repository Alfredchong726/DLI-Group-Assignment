{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## First commint to GitHub"
      ],
      "metadata": {
        "id": "eefkLYk01aJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf25uYCu8Orn",
        "outputId": "0f560279-a96c-4753-b35e-e2e4f1a19d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  DATA CLEANING & PREPROCESSING\n",
        "\n"
      ],
      "metadata": {
        "id": "AIYfbYQlprBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up & Imports"
      ],
      "metadata": {
        "id": "mqzqKc5TCHuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict\n",
        "import sys, json, glob, re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Versions -> pandas:\", pd.__version__, \"| numpy:\", np.__version__)"
      ],
      "metadata": {
        "id": "Jd4UXRmzpwuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config"
      ],
      "metadata": {
        "id": "aVaC2byzCl48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_DRIVE = True                            # Mount Google Drive to search for the file\n",
        "DRIVE_SEARCH_DIR = \"/content/drive/MyDrive\" # Root folder to search\n",
        "FILE_PATTERN = \"**/Phishing_Mendeley*.csv\"  # Pattern to locate your CSV in Drive\n",
        "FALLBACK_PROMPT_UPLOAD = True               # If not found, open an upload dialog\n",
        "\n",
        "# Behaviors\n",
        "DROP_DUPLICATES = True                      # Only drop full-row duplicates (incl. 'id'); if id differs -> keep\n",
        "DROP_HIGH_MISSING_COLS = False              # If True, drop cols with missing rate > HIGH_MISSING_THRESHOLD\n",
        "HIGH_MISSING_THRESHOLD = 0.40\n",
        "\n",
        "# Save to Drive too?\n",
        "SAVE_BACK_TO_DRIVE = False                  # If True, also copy outputs to DRIVE_OUT_DIR\n",
        "DRIVE_OUT_DIR = \"/content/drive/MyDrive/phishing_cleaned_outputs\""
      ],
      "metadata": {
        "id": "x5hF-nyiCmVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive/Load Utilities"
      ],
      "metadata": {
        "id": "-rKW50bkCn2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive_if_needed():\n",
        "    if USE_DRIVE:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Drive mounted.\")\n",
        "        except Exception as e:\n",
        "            print(\"Drive mount failed or not in Colab:\", e)\n",
        "\n",
        "def find_csv_in_drive(search_dir: str, pattern: str) -> Optional[str]:\n",
        "    paths = glob.glob(str(Path(search_dir) / pattern), recursive=True)\n",
        "    return max(paths, key=lambda p: Path(p).stat().st_mtime) if paths else None\n",
        "\n",
        "def upload_dialog() -> Optional[str]:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Please upload your CSV file…\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            return None\n",
        "        name = next(iter(uploaded.keys()))\n",
        "        print(\"Uploaded:\", name)\n",
        "        return str(Path(\"/content\") / name)\n",
        "    except Exception as e:\n",
        "        print(\"Upload dialog not available (not in Colab?):\", e)\n",
        "        return None\n",
        "\n",
        "def read_raw(p: str | Path) -> pd.DataFrame:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return pd.read_csv(p, encoding=enc, engine=\"python\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise RuntimeError(\"Failed to read CSV with utf-8 / utf-8-sig / latin-1.\")"
      ],
      "metadata": {
        "id": "9FfiynjkCq_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Locate & Load the CSV"
      ],
      "metadata": {
        "id": "CRti-H5YCtFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mount_drive_if_needed()\n",
        "\n",
        "csv_path = None\n",
        "if USE_DRIVE:\n",
        "    csv_path = find_csv_in_drive(DRIVE_SEARCH_DIR, FILE_PATTERN)\n",
        "    print(\"Drive search:\", \"FOUND\" if csv_path else \"Not found\")\n",
        "\n",
        "if not csv_path and FALLBACK_PROMPT_UPLOAD:\n",
        "    csv_path = upload_dialog()\n",
        "\n",
        "if not csv_path:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not locate a dataset. Set USE_DRIVE=True with correct DRIVE_SEARCH_DIR/FILE_PATTERN \"\n",
        "        \"or enable FALLBACK_PROMPT_UPLOAD.\"\n",
        "    )\n",
        "\n",
        "print(\"Using dataset:\", csv_path)\n",
        "\n",
        "raw_df = read_raw(csv_path)\n",
        "orig_shape = (int(raw_df.shape[0]), int(raw_df.shape[1]))\n",
        "\n",
        "# Preserve CamelCase names; trim whitespace inside string cells\n",
        "df = raw_df.copy()\n",
        "for c in df.select_dtypes(include=[object]).columns:\n",
        "    df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "print(\"Loaded shape:\", df.shape)"
      ],
      "metadata": {
        "id": "ntWEv5kaCtu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info"
      ],
      "metadata": {
        "id": "axdgtcpOCu2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from textwrap import indent\n",
        "\n",
        "print(\"\\n=== DATASET INFO ===\")\n",
        "print(\"Path:\", csv_path)\n",
        "print(\"Original shape:\", orig_shape)\n",
        "\n",
        "print(\"\\n.dtypes (first 50):\")\n",
        "print(df.dtypes.head(50))\n",
        "\n",
        "print(\"\\n.info():\")\n",
        "# Capture df.info() into a string buffer so we can print it nicely\n",
        "buffer = io.StringIO()\n",
        "df.info(buf=buffer)\n",
        "info_str = buffer.getvalue()\n",
        "print(info_str)\n",
        "\n",
        "print(\"\\n.head(5):\")\n",
        "display(df.head(5))\n",
        "\n",
        "print(\"\\nMissingness (top 20):\")\n",
        "miss = df.isna().mean().sort_values(ascending=False)\n",
        "display(miss.head(20).to_frame(\"missing_rate\"))\n",
        "\n",
        "# Early guess of target column (just for info; final alignment happens later)\n",
        "target_guess = next(\n",
        "    (c for c in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]\n",
        "     if c in df.columns),\n",
        "    None\n",
        ")\n",
        "print(\"\\nTarget column guess:\", target_guess)\n",
        "if target_guess is not None:\n",
        "    # Show a small sample of unique values\n",
        "    try:\n",
        "        uniques = pd.unique(df[target_guess].dropna())\n",
        "        print(\"Sample target values:\", uniques[:10])\n",
        "    except Exception as e:\n",
        "        print(\"Could not preview target values:\", e)"
      ],
      "metadata": {
        "id": "Z6WPc6GSCx7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions for Cleaning"
      ],
      "metadata": {
        "id": "rZbpPFIXCzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coerce_numeric_like(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Coerce object columns that look numeric (>=80% numeric-like) into numeric dtype.\"\"\"\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            s = df[c].astype(str).str.strip()\n",
        "            mask = s.str.match(r'^[+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?$')\n",
        "            if mask.mean() >= 0.8:\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def handle_infinities(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def drop_low_variance(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, list]:\n",
        "    nunique = df.nunique(dropna=False)\n",
        "    lowvar = nunique[nunique <= 1].index.tolist()\n",
        "    if target_col in lowvar:\n",
        "        lowvar.remove(target_col)\n",
        "    if lowvar:\n",
        "        df = df.drop(columns=lowvar)\n",
        "    return df, lowvar\n",
        "\n",
        "def impute_missing(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, Dict]:\n",
        "    df = df.copy()\n",
        "    report: Dict = {}\n",
        "\n",
        "    if DROP_HIGH_MISSING_COLS:\n",
        "        miss_rate = df.isna().mean().sort_values(ascending=False)\n",
        "        drop_cols = miss_rate[miss_rate > HIGH_MISSING_THRESHOLD].index.tolist()\n",
        "        if target_col in drop_cols:\n",
        "            drop_cols.remove(target_col)\n",
        "        if drop_cols:\n",
        "            df = df.drop(columns=drop_cols)\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": drop_cols\n",
        "        }\n",
        "    else:\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": []\n",
        "        }\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
        "    if target_col and target_col in cat_cols:\n",
        "        cat_cols.remove(target_col)\n",
        "\n",
        "    imputations = {\"numeric\": {}, \"categorical\": {}}\n",
        "    for c in num_cols:\n",
        "        if df[c].isna().any():\n",
        "            med = df[c].median()\n",
        "            df[c] = df[c].fillna(med)\n",
        "            imputations[\"numeric\"][c] = None if pd.isna(med) else float(med)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if df[c].isna().any():\n",
        "            mode = df[c].mode(dropna=True)\n",
        "            val = mode.iloc[0] if not mode.empty else \"__missing__\"\n",
        "            df[c] = df[c].fillna(val)\n",
        "            imputations[\"categorical\"][c] = val\n",
        "\n",
        "    report[\"imputations\"] = imputations\n",
        "    return df, report\n",
        "\n",
        "def finalize_int_casts(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_float_dtype(df[c]):\n",
        "            s = df[c]\n",
        "            if np.allclose(s.dropna() % 1, 0):\n",
        "                try:\n",
        "                    df[c] = s.astype(\"Int64\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return df\n",
        "\n",
        "def make_jsonable(obj):\n",
        "    import numpy as _np\n",
        "    import pandas as _pd\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_jsonable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple, set)):\n",
        "        return [make_jsonable(v) for v in obj]\n",
        "    if isinstance(obj, (_np.integer,)):\n",
        "        return int(obj)\n",
        "    if isinstance(obj, (_np.floating,)):\n",
        "        return float(obj)\n",
        "    if isinstance(obj, (_np.bool_,)):\n",
        "        return bool(obj)\n",
        "    if isinstance(obj, _np.ndarray):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.Series):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.DataFrame):\n",
        "        return obj.to_dict(orient=\"list\")\n",
        "    return obj"
      ],
      "metadata": {
        "id": "4AlaijahC1XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Pipeline"
      ],
      "metadata": {
        "id": "aO2jMAy3C2tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_df = df.copy()\n",
        "report = {\n",
        "    \"source\": csv_path,\n",
        "    \"original_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"settings\": {\n",
        "        \"DROP_DUPLICATES\": bool(DROP_DUPLICATES),\n",
        "        \"DROP_HIGH_MISSING_COLS\": bool(DROP_HIGH_MISSING_COLS),\n",
        "        \"HIGH_MISSING_THRESHOLD\": float(HIGH_MISSING_THRESHOLD),\n",
        "    },\n",
        "    \"steps\": {}\n",
        "}\n",
        "\n",
        "# Duplicates (full-row)\n",
        "dup_count = int(work_df.duplicated().sum())\n",
        "if DROP_DUPLICATES and dup_count > 0:\n",
        "    work_df = work_df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
        "report[\"steps\"][\"duplicate_rows_found_full_row\"] = dup_count\n",
        "report[\"steps\"][\"duplicates_removed\"] = int(dup_count if DROP_DUPLICATES else 0)\n",
        "print(f\"Duplicates found: {dup_count} | Removed: {report['steps']['duplicates_removed']}\")\n",
        "\n",
        "# Drop non-predictive ID AFTER dedupe\n",
        "dropped_non_predictive = []\n",
        "if \"id\" in work_df.columns:\n",
        "    work_df = work_df.drop(columns=[\"id\"])\n",
        "    dropped_non_predictive.append(\"id\")\n",
        "report[\"steps\"][\"dropped_non_predictive\"] = dropped_non_predictive\n",
        "if dropped_non_predictive:\n",
        "    print(\"Dropped columns (non-predictive):\", dropped_non_predictive)\n",
        "\n",
        "# Coerce numeric-like; handle ±inf\n",
        "work_df = coerce_numeric_like(work_df)\n",
        "work_df = handle_infinities(work_df)\n",
        "\n",
        "# Target alignment (prefer CLASS_LABEL, but auto-detect if changed)\n",
        "target_col = None\n",
        "for cand in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]:\n",
        "    if cand in work_df.columns:\n",
        "        target_col = cand\n",
        "        break\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found (expected 'CLASS_LABEL' or close variant).\")\n",
        "\n",
        "# Ensure numeric binary target\n",
        "if work_df[target_col].dtype == object:\n",
        "    y_num = pd.to_numeric(work_df[target_col], errors=\"coerce\")\n",
        "    if y_num.isna().any():\n",
        "        y_num = pd.Series(pd.factorize(work_df[target_col].astype(str).str.strip().str.lower())[0], index=work_df.index)\n",
        "    work_df[target_col] = y_num\n",
        "\n",
        "uniq = set(pd.unique(work_df[target_col].dropna()))\n",
        "if uniq.issubset({-1, 0, 1}) and uniq != {0, 1}:\n",
        "    # If dataset uses -1/1 or -1/0/1, map negatives to 1 (phishing) and non-negatives to 0\n",
        "    work_df[target_col] = work_df[target_col].map(lambda v: 1 if v < 0 else (0 if v > 0 else 0))\n",
        "\n",
        "report[\"steps\"][\"target_info\"] = {\n",
        "    \"name\": target_col,\n",
        "    \"unique_values_after_normalization\": sorted([int(x) for x in pd.unique(work_df[target_col].dropna())])\n",
        "}\n",
        "print(\"Target column:\", target_col)\n",
        "print(\"Target uniques (post-normalization):\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "\n",
        "# Drop truly constant columns (except target)\n",
        "work_df, lowvar_dropped = drop_low_variance(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"low_variance_dropped\"] = lowvar_dropped\n",
        "if lowvar_dropped:\n",
        "    print(\"Dropped low-variance cols:\", lowvar_dropped)\n",
        "\n",
        "# Impute missing values (no row dropping)\n",
        "work_df, mv_report = impute_missing(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"missing_value_handling\"] = mv_report\n",
        "print(\"Imputation summary:\", json.dumps(mv_report, indent=2)[:1000], \"...\")\n",
        "\n",
        "# Cast floats-that-are-integers to Int64\n",
        "work_df = finalize_int_casts(work_df)\n",
        "\n",
        "print(\"\\nPost-clean shape:\", work_df.shape)"
      ],
      "metadata": {
        "id": "CBXKaKKwC4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Cleaned Data & Report"
      ],
      "metadata": {
        "id": "_utgsuMEC5ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dir = Path(csv_path).parent  # same folder as original file\n",
        "csv_out = orig_dir / \"phishing_mendeley_cleaned.csv\"\n",
        "json_report = orig_dir / \"phishing_mendeley_cleaned_report.json\"\n",
        "\n",
        "# Save cleaned CSV\n",
        "work_df.to_csv(csv_out, index=False)\n",
        "\n",
        "# Update & save JSON report\n",
        "report.update({\n",
        "    \"final_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"row_delta\": int(work_df.shape[0] - orig_shape[0]),\n",
        "    \"outputs\": {\n",
        "        \"csv\": str(csv_out),\n",
        "        \"json_report\": str(json_report),\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(json_report, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump(make_jsonable(report), jf, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Saved CSV       :\", csv_out)\n",
        "print(\"Saved JSON      :\", json_report)\n",
        "\n",
        "# Optional copy back to Drive output folder\n",
        "if SAVE_BACK_TO_DRIVE and USE_DRIVE:\n",
        "    outdir = Path(DRIVE_OUT_DIR)\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    dst_csv = outdir / csv_out.name\n",
        "    dst_json = outdir / json_report.name\n",
        "    _ = Path(dst_csv).write_bytes(Path(csv_out).read_bytes())\n",
        "    _ = Path(dst_json).write_bytes(Path(json_report).read_bytes())\n",
        "    print(\"Also copied to  :\", outdir)"
      ],
      "metadata": {
        "id": "9JbkO6PPC7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Summary & Target Distribution"
      ],
      "metadata": {
        "id": "LaWqFpWIC-Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"Source          :\", report['source'])\n",
        "print(\"Original shape  :\", report['original_shape'])\n",
        "print(\"Final shape     :\", report['final_shape'])\n",
        "print(\"Row delta       :\", report['row_delta'])\n",
        "print(\"Dup (full-row)  :\", report['steps']['duplicate_rows_found_full_row'],\n",
        "      \"| removed:\", report['steps']['duplicates_removed'])\n",
        "print(\"Dropped (non-predictive):\", report['steps']['dropped_non_predictive'])\n",
        "\n",
        "print(\"\\nTarget:\", report[\"steps\"][\"target_info\"][\"name\"])\n",
        "print(\"Target uniques :\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "print(\"\\nTarget distribution:\")\n",
        "display(work_df[report[\"steps\"][\"target_info\"][\"name\"]].value_counts(dropna=False).to_frame(\"count\"))\n",
        "\n",
        "print(\"\\nPreview cleaned data:\")\n",
        "display(work_df.head(10))"
      ],
      "metadata": {
        "id": "Wzi7_2yfC90N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ MODEL TRAINING ============"
      ],
      "metadata": {
        "id": "2sJvFhP-8c_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHONG MUN SEONG (TP063440)"
      ],
      "metadata": {
        "id": "tS5HMHxv73VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & constant"
      ],
      "metadata": {
        "id": "xzh5TVWOEp-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, re, math, json, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "TOP_K = 20\n",
        "USE_THRESHOLD = False\n",
        "THRESH = 0.9935\n",
        "\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.35\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 100\n",
        "VAL_SPLIT = 0.15\n",
        "TEST_SPLIT = 0.15\n",
        "EARLY_STOP_PATIENCE = 12\n",
        "LR_SCHED_PATIENCE = 5\n",
        "LR_DECAY_FACTOR = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "wY7i46GsEooH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load cleaned csv file"
      ],
      "metadata": {
        "id": "pk6TC7j8EsNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = glob.glob(\"/content/drive/**/phishing_mendeley_cleaned.csv\", recursive=True)\n",
        "if not candidates:\n",
        "    candidates = glob.glob(\"/content/**/phishing_mendeley_cleaned.csv\", recursive=True)\n",
        "\n",
        "if not candidates:\n",
        "    candidates = glob.glob(\"/content/drive/**/Phishing_Mendeley*.csv\", recursive=True) + \\\n",
        "                 glob.glob(\"/content/**/Phishing_Mendeley*.csv\", recursive=True)\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\"Couldn't find 'phishing_mendeley_cleaned.csv'. Please set csv_path manually below.\")\n",
        "\n",
        "csv_path = max(candidates, key=lambda p: Path(p).stat().st_mtime)\n",
        "print(\"Using:\", csv_path)\n",
        "\n",
        "df = pd.read_csv(csv_path, engine=\"python\")\n",
        "print(df.shape, \"columns:\", len(df.columns))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "vqVzdp5fEurC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL preprocessing & structural feature"
      ],
      "metadata": {
        "id": "90miUtR-Ev6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_candidates = [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]\n",
        "target_col = next((c for c in target_candidates if c in df.columns), None)\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found. Please rename your label to one of: \" + \", \".join(target_candidates))\n",
        "\n",
        "y = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(0).astype(int).values\n",
        "print(\"Target:\", target_col, \"unique:\", np.unique(y, return_counts=True))\n",
        "\n",
        "url_col_candidates = [\"url\",\"URL\",\"Url\",\"URl\",\"full_url\",\"FullURL\",\"Address\",\"address\",\"Domain\",\"domain\",\"Hostname\",\"hostname\"]\n",
        "url_col = next((c for c in url_col_candidates if c in df.columns and df[c].astype(str).str.contains(r'\\.|http', na=False).any()), None)\n",
        "print(\"URL column detected:\", url_col)"
      ],
      "metadata": {
        "id": "1JWEOObWExr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL semantic features & structural features"
      ],
      "metadata": {
        "id": "3WfZxCWoEzS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_scheme_www(s: str) -> str:\n",
        "    s = re.sub(r'^\\s*https?://', '', str(s).strip(), flags=re.I)\n",
        "    s = re.sub(r'^\\s*www\\.', '', s, flags=re.I)\n",
        "    return s\n",
        "\n",
        "def is_ip_domain(host: str) -> bool:\n",
        "    return bool(re.fullmatch(r'\\d{1,3}(?:\\.\\d{1,3}){3}', host))\n",
        "\n",
        "SHORTENERS = set(\"\"\"\n",
        "bit.ly goo.gl t.co ow.ly is.gd buff.ly tinyurl.com lnkd.in rebrand.ly cutt.ly\n",
        "t.ly s.id v.gd adf.ly chilp.it clck.ru fb.me youtu.be\n",
        "\"\"\".split())\n",
        "\n",
        "def parse_host_path(url_no_scheme: str):\n",
        "    parts = url_no_scheme.split('/', 1)\n",
        "    host = parts[0].split('?')[0]\n",
        "    path = parts[1] if len(parts) > 1 else \"\"\n",
        "    return host.lower(), path\n",
        "\n",
        "def tld_prune(host: str):\n",
        "    segs = [s for s in host.split('.') if s]\n",
        "    if len(segs) >= 2:\n",
        "        core = segs[:-1]\n",
        "    else:\n",
        "        core = segs\n",
        "    if core and core[0] == 'www':\n",
        "        core = core[1:]\n",
        "    return core\n",
        "\n",
        "def subdomain_count(host: str) -> int:\n",
        "    core = tld_prune(host)\n",
        "    return max(0, len(core) - 1)\n",
        "\n",
        "def has_double_slash_in_path(path: str) -> bool:\n",
        "    return '//' in path\n",
        "\n",
        "def is_shortener(host: str) -> bool:\n",
        "    return host in SHORTENERS\n",
        "\n",
        "CHARS = [chr(i) for i in range(32, 127)]\n",
        "char2idx = {ch:i for i,ch in enumerate(CHARS)}\n",
        "EMB_DIM = 16\n",
        "rng = np.random.RandomState(SEED)\n",
        "char_emb = rng.normal(loc=0.0, scale=1.0, size=(len(CHARS), EMB_DIM)).astype(np.float32)\n",
        "\n",
        "def url_semantic_vec(url: str) -> np.ndarray:\n",
        "    if not isinstance(url, str):\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    s = strip_scheme_www(url)\n",
        "    if not s:\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    vecs = []\n",
        "    for ch in s:\n",
        "        idx = char2idx.get(ch)\n",
        "        if idx is not None:\n",
        "            vecs.append(char_emb[idx])\n",
        "    if not vecs:\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    return np.mean(np.vstack(vecs), axis=0).astype(np.float32)\n",
        "\n",
        "def url_struct_vec(url: str) -> np.ndarray:\n",
        "    if not isinstance(url, str):\n",
        "        return np.zeros(7, dtype=np.float32)\n",
        "    raw = url.strip()\n",
        "    url_noscheme = strip_scheme_www(raw)\n",
        "    host, path = parse_host_path(url_noscheme)\n",
        "\n",
        "    f_ip = 1.0 if is_ip_domain(host) else 0.0\n",
        "    f_len = float(len(raw))\n",
        "    f_short = 1.0 if is_shortener(host) else 0.0\n",
        "    f_at = 1.0 if '@' in raw else 0.0\n",
        "    f_doubleslash = 1.0 if has_double_slash_in_path(path) else 0.0\n",
        "    f_dash = 1.0 if '-' in host else 0.0\n",
        "    f_sub = float(subdomain_count(host))\n",
        "\n",
        "    return np.array([f_ip, f_len, f_short, f_at, f_doubleslash, f_dash, f_sub], dtype=np.float32)"
      ],
      "metadata": {
        "id": "LegOkgRIE1Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build feature matrix"
      ],
      "metadata": {
        "id": "YREtkx8SE3I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_semantic = url_col is not None\n",
        "\n",
        "if use_semantic:\n",
        "    urls = df[url_col].astype(str).fillna(\"\")\n",
        "    SEM = np.vstack([url_semantic_vec(u) for u in urls])\n",
        "    STR = np.vstack([url_struct_vec(u) for u in urls])\n",
        "    X = np.hstack([SEM, STR])   # 16 + 7 = 23 dims\n",
        "    feature_desc = f\"Using SEM(16) + STR(7) => {X.shape[1]} dims\"\n",
        "else:\n",
        "    possible_map = {\n",
        "        \"having_IP_Address\": None,\n",
        "        \"URL_Length\": None,\n",
        "        \"Shortining_Service\": None,\n",
        "        \"having_At_Symbol\": None,\n",
        "        \"double_slash_redirecting\": None,\n",
        "        \"Prefix_Suffix\": None,\n",
        "        \"having_Sub_Domain\": None\n",
        "    }\n",
        "    avail = [c for c in possible_map if c in df.columns]\n",
        "    if len(avail) >= 3:\n",
        "        X = df[avail].astype(float).values\n",
        "        feature_desc = f\"Structural-only from dataset columns: {avail}\"\n",
        "    else:\n",
        "        num_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        X = df[num_cols].astype(float).values\n",
        "        feature_desc = f\"No URL text or standard structural columns detected; using numeric features: {len(num_cols)} cols\"\n",
        "\n",
        "print(feature_desc)\n",
        "print(\"X shape:\", X.shape)"
      ],
      "metadata": {
        "id": "8Qsw2z6cE3fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Val/Test split"
      ],
      "metadata": {
        "id": "rVy6O5KfE5zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_all = np.arange(len(y))\n",
        "X_train, X_tmp, y_train, y_tmp, idx_train, idx_tmp = train_test_split(\n",
        "    X, y, idx_all, test_size=VAL_SPLIT + TEST_SPLIT, random_state=SEED, stratify=y)\n",
        "\n",
        "val_size = VAL_SPLIT / (VAL_SPLIT + TEST_SPLIT)\n",
        "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
        "    X_tmp, y_tmp, idx_tmp, test_size=(1 - val_size), random_state=SEED, stratify=y_tmp)\n",
        "\n",
        "print(\"Splits:\", len(idx_train), len(idx_val), len(idx_test))\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_std = scaler.transform(X)\n",
        "\n",
        "train_mask = np.zeros(len(y), dtype=bool); train_mask[idx_train] = True\n",
        "val_mask   = np.zeros(len(y), dtype=bool); val_mask[idx_val]   = True\n",
        "test_mask  = np.zeros(len(y), dtype=bool); test_mask[idx_test] = True"
      ],
      "metadata": {
        "id": "KLGGbemSE6Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Top-k cosine graph"
      ],
      "metadata": {
        "id": "crBk4JJdE7pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_topk_graph(features: np.ndarray, k: int = 12, use_threshold=False, thresh=0.9935):\n",
        "    if use_threshold:\n",
        "        nn = NearestNeighbors(n_neighbors=min(64, features.shape[0]-1), metric='cosine', algorithm='auto', n_jobs=-1)\n",
        "        nn.fit(features)\n",
        "        dists, nbrs = nn.kneighbors(features, return_distance=True)\n",
        "        sims = 1.0 - dists\n",
        "        rows, cols = [], []\n",
        "        for i in range(features.shape[0]):\n",
        "            for sim, j in zip(sims[i], nbrs[i]):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if sim >= thresh:\n",
        "                    rows.append(i); cols.append(j)\n",
        "    else:\n",
        "        nn = NearestNeighbors(n_neighbors=min(k+1, features.shape[0]), metric='cosine', algorithm='auto', n_jobs=-1)\n",
        "        nn.fit(features)\n",
        "        dists, nbrs = nn.kneighbors(features, return_distance=True)\n",
        "        rows, cols = [], []\n",
        "        for i in range(features.shape[0]):\n",
        "            for j in nbrs[i]:\n",
        "                if i == j:\n",
        "                    continue\n",
        "                rows.append(i); cols.append(j)\n",
        "\n",
        "    edges = set(zip(rows, cols))\n",
        "    edges |= set((j,i) for (i,j) in edges)\n",
        "    rows, cols = zip(*edges) if edges else ([],[])\n",
        "    return np.array(rows, dtype=np.int64), np.array(cols, dtype=np.int64)\n",
        "\n",
        "rows, cols = build_topk_graph(X_std, k=TOP_K, use_threshold=USE_THRESHOLD, thresh=THRESH)\n",
        "print(\"Edges (undirected, no self-loops):\", len(rows))"
      ],
      "metadata": {
        "id": "Q9DF1QWzE82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize adjacency and define a 2 layer GCN"
      ],
      "metadata": {
        "id": "tRegmx11E-Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_normalized_adj(n_nodes: int, rows: np.ndarray, cols: np.ndarray):\n",
        "    rows_all = np.concatenate([rows, np.arange(n_nodes)])\n",
        "    cols_all = np.concatenate([cols, np.arange(n_nodes)])\n",
        "    data = np.ones_like(rows_all, dtype=np.float32)\n",
        "\n",
        "    idx = np.vstack([rows_all, cols_all])\n",
        "    A = torch.sparse_coo_tensor(indices=idx, values=torch.from_numpy(data), size=(n_nodes, n_nodes))\n",
        "    A = A.coalesce()\n",
        "\n",
        "    deg = torch.sparse.sum(A, dim=1).to_dense()\n",
        "    deg_inv_sqrt = torch.pow(deg + 1e-8, -0.5)\n",
        "    d_i = deg_inv_sqrt[rows_all]\n",
        "    d_j = deg_inv_sqrt[cols_all]\n",
        "    norm_vals = torch.from_numpy(data) * d_i * d_j\n",
        "\n",
        "    A_norm = torch.sparse_coo_tensor(indices=idx, values=norm_vals, size=(n_nodes, n_nodes))\n",
        "    return A_norm.coalesce()\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, A_norm):\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sparse.mm(A_norm, x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim=2, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.gcn1 = GCNLayer(in_dim, hidden, dropout)\n",
        "        self.gcn2 = GCNLayer(hidden, out_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, A_norm):\n",
        "        x = self.gcn1(x, A_norm)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gcn2(x, A_norm)\n",
        "        return x\n",
        "\n",
        "X_tensor = torch.from_numpy(X_std.astype(np.float32))\n",
        "y_tensor = torch.from_numpy(y.astype(np.int64))\n",
        "A_norm = build_normalized_adj(n_nodes=X_tensor.shape[0], rows=rows, cols=cols)\n",
        "\n",
        "train_mask_t = torch.from_numpy(train_mask)\n",
        "val_mask_t   = torch.from_numpy(val_mask)\n",
        "test_mask_t  = torch.from_numpy(test_mask)\n",
        "\n",
        "X_tensor = X_tensor.to(device)\n",
        "y_tensor = y_tensor.to(device)\n",
        "A_norm   = A_norm.to(device)\n",
        "train_mask_t = train_mask_t.to(device)\n",
        "val_mask_t = val_mask_t.to(device)\n",
        "test_mask_t = test_mask_t.to(device)\n",
        "\n",
        "model = GCN(in_dim=X_tensor.shape[1], hidden=HIDDEN_DIM, out_dim=2, dropout=DROPOUT).to(device)\n",
        "\n",
        "pos_weight = float((y == 0).sum() / max(1, (y == 1).sum()))\n",
        "weights = torch.tensor([1.0, pos_weight], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "-VMnJi0EFAL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "nIn7ZWc_FBhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(split_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor, A_norm)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        y_true = y_tensor[split_mask].detach().cpu().numpy()\n",
        "        y_pred = preds[split_mask].detach().cpu().numpy()\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "        return acc, prec, rec, f1\n",
        "\n",
        "best_val = (-1, -1, -1, -1)\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X_tensor, A_norm)\n",
        "    loss = criterion(logits[train_mask_t], y_tensor[train_mask_t])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        tr = evaluate(train_mask_t)\n",
        "        va = evaluate(val_mask_t)\n",
        "        if va[3] > best_val[3]:\n",
        "            best_val = va\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | \"\n",
        "              f\"Train A/P/R/F1: {tr[0]:.3f}/{tr[1]:.3f}/{tr[2]:.3f}/{tr[3]:.3f} | \"\n",
        "              f\"Val A/P/R/F1: {va[0]:.3f}/{va[1]:.3f}/{va[2]:.3f}/{va[3]:.3f}\")\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "ta = evaluate(test_mask_t)\n",
        "print(\"\\n=== TEST METRICS (Best F1 on Val) ===\")\n",
        "print(f\"Accuracy : {ta[0]:.4f}\")\n",
        "print(f\"Precision: {ta[1]:.4f}\")\n",
        "print(f\"Recall   : {ta[2]:.4f}\")\n",
        "print(f\"F1-score : {ta[3]:.4f}\")"
      ],
      "metadata": {
        "id": "3WafpvWQFC0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize graph"
      ],
      "metadata": {
        "id": "4TUFq4eqFD0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    classification_report, roc_curve, auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _mask_np(mask_t):\n",
        "    return mask_t.detach().cpu().numpy().astype(bool)\n",
        "\n",
        "def get_split_preds(mask_t):\n",
        "    mask = _mask_np(mask_t)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor, A_norm)\n",
        "        probs1 = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "        preds = (probs1 >= 0.5).astype(int)\n",
        "    y_true = y_tensor.detach().cpu().numpy()[mask]\n",
        "    y_pred = preds[mask]\n",
        "    y_score = probs1[mask]\n",
        "    return y_true, y_pred, y_score\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, y_pred, _ = get_split_preds(m)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
        "    disp.plot(values_format=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{split_name} Confusion Matrix\")\n",
        "    plt.show()\n",
        "    print(f\"{split_name} classification report:\\n\",\n",
        "          classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "qgZ2KAHJFHXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, _, y_score = get_split_preds(m)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    axes[0].plot(fpr, tpr, label=f\"{split_name} AUC={auc(fpr,tpr):.3f}\")\n",
        "axes[0].plot([0,1],[0,1],'--',linewidth=1)\n",
        "axes[0].set_title(\"ROC Curve\"); axes[0].set_xlabel(\"FPR\"); axes[0].set_ylabel(\"TPR\"); axes[0].legend()\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, _, y_score = get_split_preds(m)\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
        "    axes[1].plot(rec, prec, label=f\"{split_name}\")\n",
        "axes[1].set_title(\"Precision–Recall Curve\"); axes[1].set_xlabel(\"Recall\"); axes[1].set_ylabel(\"Precision\"); axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYwMzSw7FMk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = X_tensor.shape[0]\n",
        "deg_out = np.zeros(n, dtype=np.float32)\n",
        "for i, w in zip(rows, weights):\n",
        "    deg_out[i] += w\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(deg_out, bins=30)\n",
        "plt.title(\"Weighted Out-Degree Distribution\")\n",
        "plt.xlabel(\"Sum of edge weights (per node)\"); plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Quh3LijvFOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOO CHEN KANG (TP065578)"
      ],
      "metadata": {
        "id": "meABj23S79Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "id3CU_91JSpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib, json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# ------------------- Paths -------------------\n",
        "MODEL_PATH = \"/content/drive/MyDrive/DLI/lightgbm_top32.pkl\"\n",
        "FEATURE_PATH = \"/content/drive/MyDrive/DLI/lightgbm_features_top32.json\"\n",
        "\n",
        "# ------------------- Load Data -------------------\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DLI/phishing_mendeley_cleaned.csv\")\n",
        "\n",
        "# ------------------- Features & Target -------------------\n",
        "target_column = 'CLASS_LABEL'\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column]\n",
        "\n",
        "# ------------------- Train/Test Split -------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ------------------- Best Parameters -------------------\n",
        "best_params = {\n",
        "    \"n_estimators\": 800,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 127,\n",
        "    \"feature_fraction\": 0.7,\n",
        "    \"bagging_fraction\": 0.6,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_child_samples\": 20,\n",
        "    \"reg_alpha\": 0,\n",
        "    \"reg_lambda\": 0,\n",
        "    \"random_state\": 42,\n",
        "    \"n_jobs\": -1\n",
        "}\n",
        "\n",
        "# ------------------- Train Model -------------------\n",
        "lgbm = LGBMClassifier(**best_params)\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "# ------------------- Evaluate -------------------\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"Precision\": precision_score(y_test, y_pred),\n",
        "    \"Recall\": recall_score(y_test, y_pred),\n",
        "    \"F1 Score\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": float(roc_auc_score(y_test, y_pred))\n",
        "}\n",
        "\n",
        "# ------------------- Feature Importance -------------------\n",
        "feat_imp = pd.Series(lgbm.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "top32_features = feat_imp.head(32)\n",
        "print(\"\\nTop-32 Features:\\n\", top32_features)\n",
        "\n",
        "# ------------------- Save Model & Features -------------------\n",
        "joblib.dump(lgbm, MODEL_PATH)\n",
        "json.dump(list(top32_features.index), open(FEATURE_PATH, \"w\"))\n",
        "\n",
        "print(f\"\\nSaved model -> {MODEL_PATH}\")\n",
        "print(f\"Saved features -> {FEATURE_PATH}\")\n",
        "\n",
        "# Convert to DataFrame for nice table view\n",
        "metrics_df = pd.DataFrame(metrics, index=[\"LightGBM (Top-32 Features)\"])\n",
        "print(\"\\n=== Final Top-32 Model Performance ===\")\n",
        "print(metrics_df)\n",
        "\n",
        "# ------------------- Confusion Matrix -------------------\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Legitimate\", \"Phishing\"],\n",
        "            yticklabels=[\"Legitimate\", \"Phishing\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HxgiycEeJRje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance"
      ],
      "metadata": {
        "id": "xCJglVCaJVAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Feature importance ---\n",
        "feat_imp = pd.Series(lgbm.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "top32_features = feat_imp.head(32)\n",
        "\n",
        "# --- Plot heatmap ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    top32_features.to_frame(name=\"Importance\"),\n",
        "    annot=True, fmt=\"d\", cmap=\"YlGnBu\", cbar=True\n",
        ")\n",
        "plt.title(\"Top-32 Feature Importance (LightGBM)\", fontsize=14)\n",
        "plt.ylabel(\"Features\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BcBbJrO5JUzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENG YI LING (TP065686)"
      ],
      "metadata": {
        "id": "x_b5p5YN7-NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN ENSEMBLE"
      ],
      "metadata": {
        "id": "9sRatc0QGlPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# DUAL-CHANNEL CNN ENSEMBLE FOR PHISHING DETECTION\n",
        "# Grid-based CNN with MinMax + StandardScaler channels\n",
        "# ==========================================================\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import os, pickle, gc\n",
        "from math import ceil, sqrt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "DATA_PATH = \"/content/drive/**/phishing_mendeley_cleaned.csv\"\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE  = 0.2      # from training portion only (for threshold search)\n",
        "RANDOM_STATE = 42\n",
        "N_SEEDS = 5          # ensemble size (try 7 if close to 99%)\n",
        "\n",
        "# Model/training\n",
        "EPOCHS = 60\n",
        "BATCH  = 128\n",
        "INIT_LR = 2e-3\n",
        "PATIENCE_ES = 8\n",
        "PATIENCE_LR = 3\n",
        "DROPOUT_D = 0.5\n",
        "\n",
        "# Capacity (small bump is fine)\n",
        "F1, F2, F3 = 64, 96, 160   # try F3=192 if close\n",
        "D1, D2     = 256, 128      # try D1=320 if close\n",
        "\n",
        "print(\"🚀 Starting CNN Training with Saving...\")\n",
        "\n",
        "# ---------------- Load & split ----------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "assert \"CLASS_LABEL\" in df.columns\n",
        "y = df[\"CLASS_LABEL\"].astype(int).values\n",
        "Xdf = df.drop(columns=[\"CLASS_LABEL\"]).select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_full_tr_df, X_te_df, y_full_tr, y_te = train_test_split(\n",
        "    Xdf, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# train/val split (for threshold tuning)\n",
        "X_tr_df, X_val_df, y_tr, y_val = train_test_split(\n",
        "    X_full_tr_df, y_full_tr, test_size=VAL_SIZE, stratify=y_full_tr, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Data split: Train={len(y_tr)}, Val={len(y_val)}, Test={len(y_te)}\")\n",
        "\n",
        "# ---------------- Build PAD_7x7 two-channel grids ----------------\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "def next_square(n: int):\n",
        "    side = int(ceil(sqrt(n)))\n",
        "    return side*side, side\n",
        "\n",
        "def to_grid_two_channel(X2d, side, pad_to):\n",
        "    n, d = X2d.shape\n",
        "    if d < pad_to:\n",
        "        X2d = np.pad(X2d, ((0,0),(0, pad_to - d)), mode='constant')\n",
        "    mm = MinMaxScaler()\n",
        "    zc = StandardScaler()\n",
        "    X_mm = mm.fit_transform(X2d)\n",
        "    X_z  = zc.fit_transform(X2d)\n",
        "    G1 = X_mm.reshape(n, side, side, 1)\n",
        "    G2 = X_z.reshape(n, side, side, 1)\n",
        "    return np.concatenate([G1, G2], axis=-1)  # (n, H, W, 2)\n",
        "\n",
        "d = Xdf.shape[1]\n",
        "pad_cells, side = next_square(d)  # 47 -> 49, side=7\n",
        "print(f\"PAD grid: {side}x{side} (pad {pad_cells - d} zeros)\")\n",
        "\n",
        "X_tr_g  = to_grid_two_channel(X_tr_df.values,  side, pad_cells)\n",
        "X_val_g = to_grid_two_channel(X_val_df.values, side, pad_cells)\n",
        "X_te_g  = to_grid_two_channel(X_te_df.values,  side, pad_cells)\n",
        "\n",
        "print(f\"Grid shapes: Train={X_tr_g.shape}, Val={X_val_g.shape}, Test={X_te_g.shape}\")\n",
        "\n",
        "# ---------------- Model builder ----------------\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Activation,\n",
        "                                     MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "def build_cnn(input_shape, seed):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = inp\n",
        "    for filters, pool in [(F1, True), (F2, True), (F3, False)]:\n",
        "        x = Conv2D(filters, (3,3), padding=\"same\", use_bias=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        if pool:\n",
        "            x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(D1, activation=\"relu\")(x); x = BatchNormalization()(x); x = Dropout(DROPOUT_D)(x)\n",
        "\n",
        "    # IMPORTANT: Name this layer for feature extraction\n",
        "    feature_dense = Dense(D2, activation=\"relu\", name='feature_dense')(x)\n",
        "    x = BatchNormalization()(feature_dense)\n",
        "    x = Dropout(DROPOUT_D)(x)\n",
        "\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "    m = tf.keras.Model(inputs=inp, outputs=out)\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=INIT_LR),\n",
        "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return m\n",
        "\n",
        "# ---------------- Train N seeds & collect val/test probabilities ----------------\n",
        "val_probs_list, te_probs_list = [], []\n",
        "trained_models = []  # Store trained models for saving\n",
        "\n",
        "for i, seed in enumerate([RANDOM_STATE + k for k in range(N_SEEDS)], 1):\n",
        "    print(f\"\\n=== Training seed {seed} ({i}/{N_SEEDS}) ===\")\n",
        "    model = build_cnn(X_tr_g.shape[1:], seed)\n",
        "    es  = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE_ES, restore_best_weights=True, verbose=1)\n",
        "    rlr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=PATIENCE_LR, min_lr=5e-6, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_tr_g, y_tr, validation_data=(X_val_g, y_val),\n",
        "        epochs=EPOCHS, batch_size=BATCH, callbacks=[es, rlr], verbose=1\n",
        "    )\n",
        "\n",
        "    val_probs_list.append(model.predict(X_val_g, verbose=0).ravel())\n",
        "    te_probs_list.append(model.predict(X_te_g,  verbose=0).ravel())\n",
        "\n",
        "    # Store the trained model\n",
        "    trained_models.append(model)\n",
        "\n",
        "    print(f\"✅ Model {i} training completed\")\n",
        "\n",
        "    # Memory cleanup\n",
        "    del history\n",
        "    gc.collect()\n",
        "\n",
        "print(\"🎉 All CNN models trained!\")\n",
        "\n",
        "# Ensemble by averaging probabilities\n",
        "val_proba_ens = np.mean(np.stack(val_probs_list, axis=0), axis=0)\n",
        "te_proba_ens  = np.mean(np.stack(te_probs_list,  axis=0), axis=0)\n",
        "\n",
        "# ---------------- Threshold search on validation ----------------\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        "\n",
        "ths = np.linspace(0.3, 0.7, 401)  # dense sweep\n",
        "val_accs = []\n",
        "for t in ths:\n",
        "    val_pred = (val_proba_ens >= t).astype(int)\n",
        "    val_accs.append(accuracy_score(y_val, val_pred))\n",
        "best_idx = int(np.argmax(val_accs))\n",
        "best_th = float(ths[best_idx])\n",
        "print(f\"\\nBest threshold on validation: {best_th:.4f} (val accuracy={val_accs[best_idx]:.4f})\")\n",
        "\n",
        "# ---------------- Final test metrics at best threshold ----------------\n",
        "y_pred = (te_proba_ens >= best_th).astype(int)\n",
        "acc  = accuracy_score(y_te, y_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"binary\", zero_division=0)\n",
        "auc  = roc_auc_score(y_te, te_proba_ens)\n",
        "\n",
        "print(f\"\\n🎯 CNN ENSEMBLE RESULTS:\")\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision:     {prec:.4f}\")\n",
        "print(f\"Recall:        {rec:.4f}\")\n",
        "print(f\"F1-Score:      {f1:.4f}\")\n",
        "print(f\"ROC-AUC:       {auc:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_te, y_pred, digits=4))\n",
        "cm = confusion_matrix(y_te, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# ---------------- Plots ----------------\n",
        "# ROC\n",
        "fpr, tpr, _ = roc_curve(y_te, te_proba_ens)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(fpr, tpr, label=f\"AUC={auc:.4f}\")\n",
        "plt.plot([0,1],[0,1],'--', alpha=0.6)\n",
        "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (CNN ensemble)\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# CM\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (CNN ensemble)\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "for (i,j), v in np.ndenumerate(cm):\n",
        "    plt.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "plt.colorbar(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# SAVE CNN MODELS AND ARTIFACTS\n",
        "# ==========================================================\n",
        "print(\"\\n💾 Saving CNN models and artifacts...\")\n",
        "\n",
        "# Create save directory\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/cnn_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save each trained CNN model\n",
        "cnn_model_paths = []\n",
        "seeds_used = [RANDOM_STATE + k for k in range(N_SEEDS)]\n",
        "\n",
        "for i, (model, seed) in enumerate(zip(trained_models, seeds_used)):\n",
        "    model_path = os.path.join(save_dir, f\"cnn_model_seed_{seed}.h5\")\n",
        "    model.save(model_path)\n",
        "    cnn_model_paths.append(model_path)\n",
        "    print(f\"✅ Saved CNN model {i+1}: cnn_model_seed_{seed}.h5\")\n",
        "\n",
        "# Save preprocessing parameters and metadata\n",
        "cnn_metadata = {\n",
        "    'grid_side': side,\n",
        "    'pad_cells': pad_cells,\n",
        "    'original_features': d,\n",
        "    'n_seeds': N_SEEDS,\n",
        "    'seeds_used': seeds_used,\n",
        "    'model_paths': cnn_model_paths,\n",
        "    'input_shape': X_tr_g.shape[1:],\n",
        "    'architecture_params': {\n",
        "        'F1': F1, 'F2': F2, 'F3': F3,\n",
        "        'D1': D1, 'D2': D2,\n",
        "        'dropout': DROPOUT_D\n",
        "    },\n",
        "    'training_params': {\n",
        "        'epochs': EPOCHS,\n",
        "        'batch_size': BATCH,\n",
        "        'learning_rate': INIT_LR,\n",
        "        'patience_es': PATIENCE_ES,\n",
        "        'patience_lr': PATIENCE_LR\n",
        "    },\n",
        "    'performance': {\n",
        "        'best_threshold': best_th,\n",
        "        'test_accuracy': acc,\n",
        "        'test_precision': prec,\n",
        "        'test_recall': rec,\n",
        "        'test_f1': f1,\n",
        "        'test_auc': auc\n",
        "    },\n",
        "    'ensemble_predictions': {\n",
        "        'val_probabilities': val_proba_ens.tolist(),\n",
        "        'test_probabilities': te_proba_ens.tolist()\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(save_dir, \"cnn_metadata.pkl\")\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(cnn_metadata, f)\n",
        "\n",
        "print(f\"✅ Saved CNN metadata: cnn_metadata.pkl\")\n",
        "\n",
        "# Save individual model predictions for analysis\n",
        "predictions_data = {\n",
        "    'val_predictions_individual': val_probs_list,\n",
        "    'test_predictions_individual': te_probs_list,\n",
        "    'val_ensemble': val_proba_ens,\n",
        "    'test_ensemble': te_proba_ens,\n",
        "    'y_val': y_val,\n",
        "    'y_test': y_te,\n",
        "    'best_threshold': best_th\n",
        "}\n",
        "\n",
        "predictions_path = os.path.join(save_dir, \"cnn_predictions.pkl\")\n",
        "with open(predictions_path, 'wb') as f:\n",
        "    pickle.dump(predictions_data, f)\n",
        "\n",
        "print(f\"✅ Saved CNN predictions: cnn_predictions.pkl\")\n",
        "\n",
        "# Save a summary report\n",
        "summary_report = f\"\"\"\n",
        "CNN ENSEMBLE TRAINING SUMMARY\n",
        "============================\n",
        "\n",
        "Dataset: {DATA_PATH}\n",
        "Data Shape: {df.shape}\n",
        "Features: {d}\n",
        "Grid Size: {side}x{side} (padded to {pad_cells})\n",
        "\n",
        "Architecture:\n",
        "- Conv Filters: {F1}, {F2}, {F3}\n",
        "- Dense Units: {D1}, {D2}\n",
        "- Dropout: {DROPOUT_D}\n",
        "\n",
        "Training:\n",
        "- Ensemble Size: {N_SEEDS} models\n",
        "- Epochs: {EPOCHS}\n",
        "- Batch Size: {BATCH}\n",
        "- Learning Rate: {INIT_LR}\n",
        "\n",
        "Performance:\n",
        "- Test Accuracy: {acc:.4f}\n",
        "- Test Precision: {prec:.4f}\n",
        "- Test Recall: {rec:.4f}\n",
        "- Test F1-Score: {f1:.4f}\n",
        "- Test AUC: {auc:.4f}\n",
        "- Optimal Threshold: {best_th:.4f}\n",
        "\n",
        "Models saved to: {save_dir}\n",
        "\"\"\"\n",
        "\n",
        "report_path = os.path.join(save_dir, \"training_summary.txt\")\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(f\"✅ Saved training summary: training_summary.txt\")\n",
        "\n",
        "print(\"\\n🎉 CNN TRAINING AND SAVING COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"📁 All artifacts saved to: {save_dir}\")\n",
        "print(f\"📊 Final Test Accuracy: {acc:.4f}\")\n",
        "print(\"🔄 Ready for hybrid training!\")\n",
        "\n",
        "# Memory cleanup\n",
        "del trained_models, val_probs_list, te_probs_list\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(\"✅ Memory cleaned up successfully!\")"
      ],
      "metadata": {
        "id": "m5TXcPMpGwW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "by3vHajdHAaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ (XGBOOST) ============"
      ],
      "metadata": {
        "id": "sXaTWech89Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# XGBOOST CLASSIFIER WITH AUTOMATED FEATURE SELECTION\n",
        "# MI-scoring and grid search for optimal feature subset\n",
        "# ==========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load your dataset (Colab + Drive)\n",
        "# ----------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/**/phishing_mendeley_cleaned.csv\"  # update if needed\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Columns:\", data.columns.tolist())\n",
        "print(\"Label distribution:\\n\", data['CLASS_LABEL'].value_counts())\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Feature selection (MI scores)\n",
        "# ----------------------------\n",
        "X = data.drop(['CLASS_LABEL'], axis=1)\n",
        "y = data['CLASS_LABEL']\n",
        "\n",
        "# Keep same logic as the researcher (treat dtype==int as discrete)\n",
        "discrete_features = X.dtypes == int\n",
        "\n",
        "mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n",
        "mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\nTop 10 MI features:\\n\", mi_scores.head(10))\n",
        "\n",
        "# Plot MI scores (optional)\n",
        "def plot_mi_scores(scores):\n",
        "    scores = scores.sort_values(ascending=True)\n",
        "    width = np.arange(len(scores))\n",
        "    ticks = list(scores.index)\n",
        "    plt.barh(width, scores)\n",
        "    plt.yticks(width, ticks)\n",
        "    plt.title(\"MI Scores\")\n",
        "\n",
        "plt.figure(dpi=100, figsize=(12,12))\n",
        "plot_mi_scores(mi_scores)\n",
        "plt.show()\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Training function (researcher's original logic)\n",
        "# ----------------------------\n",
        "def train_XGBC(df_in, top_n):\n",
        "    top_n_features = mi_scores.head(top_n).index.tolist()\n",
        "    Xn = df_in[top_n_features]\n",
        "    yn = df_in['CLASS_LABEL']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        Xn, yn, test_size=0.2, shuffle=True, random_state=42\n",
        "    )\n",
        "\n",
        "    model = XGBClassifier()  # default params, no early stopping (matches researcher)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    recall    = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1        = f1_score(y_test, y_pred, zero_division=0)\n",
        "    accuracy  = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Run experiments with top 20–50 features\n",
        "# ----------------------------\n",
        "arr = []\n",
        "for i in range(20, 51):\n",
        "    precision, recall, f1, accuracy = train_XGBC(data, i)\n",
        "    print(f\"Top {i} features → Precision: {precision:.4f}, Recall: {recall:.4f}, \"\n",
        "          f\"F1: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    arr.append([i, precision, recall, f1, accuracy])\n",
        "\n",
        "# Results DataFrame\n",
        "df_results = pd.DataFrame(arr, columns=['num_of_features', 'precision', 'recall', 'f1_score', 'accuracy'])\n",
        "print(\"\\n=== Results (head) ===\")\n",
        "print(df_results.head())\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Pick the best Top-N by accuracy\n",
        "# ----------------------------\n",
        "best_row = df_results.loc[df_results['accuracy'].idxmax()]\n",
        "best_topn = int(best_row['num_of_features'])\n",
        "\n",
        "print(\"\\n=== Best configuration ===\")\n",
        "print(f\"Top {best_topn} features\")\n",
        "print(f\"Precision : {best_row['precision']:.4f}\")\n",
        "print(f\"Recall    : {best_row['recall']:.4f}\")\n",
        "print(f\"F1 score  : {best_row['f1_score']:.4f}\")\n",
        "print(f\"Accuracy  : {best_row['accuracy']:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Retrain final model on ALL data using the best Top-N\n",
        "#    (common practice after model/feature selection)\n",
        "# ----------------------------\n",
        "best_features = mi_scores.head(best_topn).index.tolist()\n",
        "X_final = data[best_features]\n",
        "y_final = data['CLASS_LABEL']\n",
        "\n",
        "final_model = XGBClassifier()\n",
        "final_model.fit(X_final, y_final)\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Save artifacts to Google Drive (.pkl, .csv, .txt) - FIXED SYNTAX\n",
        "# ----------------------------\n",
        "out_dir = \"/content/drive/MyDrive/Colab Notebooks/xgb_models\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(out_dir, f\"xgb_phishing_top{best_topn}.pkl\")\n",
        "with open(model_path, \"wb\") as f:\n",
        "    # FIXED: Added missing comma after mi_scores line\n",
        "    pickle.dump(\n",
        "        {\n",
        "            \"model\": final_model,\n",
        "            \"selected_features\": best_features,\n",
        "            \"mi_scores\": mi_scores.to_dict(),  # ← FIXED: Added comma here\n",
        "            \"best_topn\": best_topn,\n",
        "            \"performance\": {\n",
        "                \"precision\": best_row['precision'],\n",
        "                \"recall\": best_row['recall'],\n",
        "                \"f1_score\": best_row['f1_score'],\n",
        "                \"accuracy\": best_row['accuracy']\n",
        "            },\n",
        "            \"training_info\": {\n",
        "                \"data_shape\": data.shape,\n",
        "                \"feature_range_tested\": \"20-50\",\n",
        "                \"random_state\": 42\n",
        "            }\n",
        "        },\n",
        "        f\n",
        "    )\n",
        "\n",
        "results_csv_path = os.path.join(out_dir, \"xgb_mi_sweep_results.csv\")\n",
        "df_results.to_csv(results_csv_path, index=False)\n",
        "\n",
        "features_txt_path = os.path.join(out_dir, f\"selected_features_top{best_topn}.txt\")\n",
        "pd.Series(best_features, name=\"feature\").to_csv(features_txt_path, index=False)\n",
        "\n",
        "# Save XGBoost metadata for hybrid training\n",
        "xgb_metadata = {\n",
        "    'best_topn': best_topn,\n",
        "    'selected_features': best_features,\n",
        "    'mi_scores_dict': mi_scores.to_dict(),\n",
        "    'model_path': model_path,\n",
        "    'data_shape': data.shape,\n",
        "    'performance': dict(best_row),\n",
        "    'all_results': df_results.to_dict('records')\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(out_dir, \"xgb_metadata.pkl\")\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(xgb_metadata, f)\n",
        "\n",
        "print(f\"\\n💾 XGBOOST ARTIFACTS SAVED:\")\n",
        "print(f\"✅ Model (.pkl): {os.path.basename(model_path)}\")\n",
        "print(f\"✅ Results (.csv): {os.path.basename(results_csv_path)}\")\n",
        "print(f\"✅ Features (.txt): {os.path.basename(features_txt_path)}\")\n",
        "print(f\"✅ Metadata (.pkl): xgb_metadata.pkl\")\n",
        "print(f\"📁 All saved to: {out_dir}\")\n",
        "\n",
        "print(f\"\\n🎯 XGBOOST TRAINING COMPLETED!\")\n",
        "print(f\"Best accuracy: {best_row['accuracy']:.4f} with {best_topn} features\")\n",
        "print(\"🔄 Ready for hybrid training!\")\n",
        "\n",
        "# ----------------------------\n",
        "# 8. (Optional) How to load and use the saved model later\n",
        "# ----------------------------\n",
        "# with open(model_path, \"rb\") as f:\n",
        "#     bundle = pickle.load(f)\n",
        "# loaded_model = bundle[\"model\"]\n",
        "# selected_feats = bundle[\"selected_features\"]\n",
        "# # Example: preds on new data (DataFrame with same columns)\n",
        "# # y_new_pred = loaded_model.predict(new_df[selected_feats])"
      ],
      "metadata": {
        "id": "_Ife0CmG9Ff5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL VERSION OF HYBRID (CNN + XGBOOST )"
      ],
      "metadata": {
        "id": "8A6NmOEcHFuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# STRATEGY 1 & 2: SELECTIVE HYBRID + WEIGHTED ENSEMBLE\n",
        "# ==========================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, pickle, gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, classification_report\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import pearsonr\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"🎯 Starting Strategy 1 & 2: Selective Hybrid + Weighted Ensemble\")\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD YOUR EXISTING MODELS AND DATA\n",
        "# ==========================================================\n",
        "\n",
        "# Load CNN models\n",
        "CNN_MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/cnn_models\"\n",
        "cnn_metadata_path = os.path.join(CNN_MODEL_DIR, \"cnn_metadata.pkl\")\n",
        "\n",
        "with open(cnn_metadata_path, 'rb') as f:\n",
        "    cnn_metadata = pickle.load(f)\n",
        "\n",
        "cnn_models = []\n",
        "for model_path in cnn_metadata['model_paths']:\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    cnn_models.append(model)\n",
        "\n",
        "print(f\"✅ Loaded {len(cnn_models)} CNN models (Test accuracy: {cnn_metadata['performance']['test_accuracy']:.4f})\")\n",
        "\n",
        "# Load XGBoost model\n",
        "XGB_MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/xgb_models\"\n",
        "xgb_files = [f for f in os.listdir(XGB_MODEL_DIR) if f.startswith('xgb_phishing_top') and f.endswith('.pkl')]\n",
        "xgb_path = os.path.join(XGB_MODEL_DIR, xgb_files[0])\n",
        "\n",
        "with open(xgb_path, 'rb') as f:\n",
        "    xgb_bundle = pickle.load(f)\n",
        "\n",
        "selected_features = xgb_bundle['selected_features']\n",
        "print(f\"✅ Loaded XGBoost model with {len(selected_features)} features\")\n",
        "\n",
        "# Load data (same split as your models)\n",
        "DATA_PATH = \"/content/drive/**/phishing_mendeley_cleaned.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "y = df[\"CLASS_LABEL\"].astype(int).values\n",
        "X_tabular = df.drop(columns=[\"CLASS_LABEL\"]).select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "X_tab_train, X_tab_test, y_train, y_test = train_test_split(\n",
        "    X_tabular, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"✅ Data loaded: {len(y_train)} train, {len(y_test)} test\")\n",
        "\n",
        "# Create grids\n",
        "side = cnn_metadata['grid_side']\n",
        "pad_cells = cnn_metadata['pad_cells']\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from math import ceil, sqrt\n",
        "\n",
        "def to_grid_two_channel(X2d, side, pad_to):\n",
        "    n, d = X2d.shape\n",
        "    if d < pad_to:\n",
        "        X2d = np.pad(X2d, ((0,0),(0, pad_to - d)), mode='constant')\n",
        "    mm = MinMaxScaler()\n",
        "    zc = StandardScaler()\n",
        "    X_mm = mm.fit_transform(X2d)\n",
        "    X_z  = zc.fit_transform(X2d)\n",
        "    G1 = X_mm.reshape(n, side, side, 1)\n",
        "    G2 = X_z.reshape(n, side, side, 1)\n",
        "    return np.concatenate([G1, G2], axis=-1)\n",
        "\n",
        "X_train_grid = to_grid_two_channel(X_tab_train.values, side, pad_cells)\n",
        "X_test_grid = to_grid_two_channel(X_tab_test.values, side, pad_cells)\n",
        "\n",
        "# ==========================================================\n",
        "# EXTRACT CNN FEATURES FROM YOUR MODELS\n",
        "# ==========================================================\n",
        "print(\"\\n🔍 Extracting CNN features...\")\n",
        "\n",
        "def create_feature_extractor(cnn_model):\n",
        "    for layer in cnn_model.layers:\n",
        "        if 'dense' in layer.name.lower() and hasattr(layer, 'units') and layer.units == 128:\n",
        "            return tf.keras.Model(inputs=cnn_model.input, outputs=layer.output)\n",
        "    return None\n",
        "\n",
        "feature_extractors = [create_feature_extractor(model) for model in cnn_models]\n",
        "feature_extractors = [e for e in feature_extractors if e is not None]\n",
        "\n",
        "def extract_cnn_features(X_grid):\n",
        "    features_list = []\n",
        "    for extractor in feature_extractors:\n",
        "        features = extractor.predict(X_grid, batch_size=64, verbose=0)\n",
        "        features_list.append(features)\n",
        "    return np.mean(np.stack(features_list, axis=0), axis=0)\n",
        "\n",
        "cnn_features_train = extract_cnn_features(X_train_grid)\n",
        "cnn_features_test = extract_cnn_features(X_test_grid)\n",
        "\n",
        "X_tab_train_selected = X_tab_train[selected_features]\n",
        "X_tab_test_selected = X_tab_test[selected_features]\n",
        "\n",
        "print(f\"✅ Features ready: {len(selected_features)} tabular + {cnn_features_train.shape[1]} CNN\")\n",
        "\n",
        "# ==========================================================\n",
        "# STRATEGY 1: SELECTIVE FEATURE FUSION\n",
        "# ==========================================================\n",
        "print(\"\\n🎯 STRATEGY 1: Selective Feature Fusion\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def select_best_cnn_features(cnn_features, tabular_features, labels, n_select=20):\n",
        "    \"\"\"Select CNN features that add most value\"\"\"\n",
        "\n",
        "    print(f\"Selecting top {n_select} CNN features out of {cnn_features.shape[1]}...\")\n",
        "\n",
        "    # Method 1: Mutual Information of CNN features\n",
        "    mi_cnn = mutual_info_classif(cnn_features, labels)\n",
        "    mi_cnn_series = pd.Series(mi_cnn, index=[f'cnn_{i}' for i in range(len(mi_cnn))])\n",
        "    top_cnn_indices = mi_cnn_series.nlargest(n_select).index\n",
        "    top_cnn_indices = [int(idx.split('_')[1]) for idx in top_cnn_indices]\n",
        "\n",
        "    print(f\"Selected CNN features by MI: {top_cnn_indices[:5]}...\")\n",
        "\n",
        "    return cnn_features[:, top_cnn_indices], top_cnn_indices\n",
        "\n",
        "# Test different numbers of CNN features\n",
        "cnn_feature_counts = [15, 20, 25, 30]\n",
        "selective_results = {}\n",
        "\n",
        "for n_cnn in cnn_feature_counts:\n",
        "    print(f\"\\n🧪 Testing with {n_cnn} CNN features...\")\n",
        "\n",
        "    # Select best CNN features\n",
        "    cnn_selected_train, selected_indices = select_best_cnn_features(\n",
        "        cnn_features_train, X_tab_train_selected, y_train, n_select=n_cnn\n",
        "    )\n",
        "    cnn_selected_test = cnn_features_test[:, selected_indices]\n",
        "\n",
        "    # Create selective hybrid features\n",
        "    X_selective_train = np.concatenate([X_tab_train_selected.values, cnn_selected_train], axis=1)\n",
        "    X_selective_test = np.concatenate([X_tab_test_selected.values, cnn_selected_test], axis=1)\n",
        "\n",
        "    # Split for validation\n",
        "    X_sel_tr, X_sel_val, y_tr, y_val = train_test_split(\n",
        "        X_selective_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train selective hybrid XGBoost\n",
        "    selective_xgb = xgb.XGBClassifier(\n",
        "        n_estimators=1000,\n",
        "        max_depth=6,  # Reduced complexity for fewer features\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_alpha=0.05,  # Light regularization\n",
        "        random_state=42,\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "\n",
        "    selective_xgb.fit(\n",
        "        X_sel_tr, y_tr,\n",
        "        eval_set=[(X_sel_val, y_val)],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Predict and evaluate\n",
        "    sel_pred_proba = selective_xgb.predict_proba(X_selective_test)[:, 1]\n",
        "    sel_pred = (sel_pred_proba >= 0.5).astype(int)\n",
        "    sel_accuracy = accuracy_score(y_test, sel_pred)\n",
        "\n",
        "    # Optimize threshold\n",
        "    thresholds = np.linspace(0.3, 0.7, 101)\n",
        "    best_acc = 0\n",
        "    best_thresh = 0.5\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        acc = accuracy_score(y_test, (sel_pred_proba >= thresh).astype(int))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_thresh = thresh\n",
        "\n",
        "    selective_results[n_cnn] = {\n",
        "        'accuracy': sel_accuracy,\n",
        "        'optimized_accuracy': best_acc,\n",
        "        'best_threshold': best_thresh,\n",
        "        'model': selective_xgb,\n",
        "        'predictions': sel_pred_proba\n",
        "    }\n",
        "\n",
        "    print(f\"   Accuracy (0.5): {sel_accuracy:.4f}\")\n",
        "    print(f\"   Optimized:      {best_acc:.4f} (thresh: {best_thresh:.3f})\")\n",
        "\n",
        "# Find best selective approach\n",
        "best_selective_n = max(selective_results.keys(), key=lambda k: selective_results[k]['optimized_accuracy'])\n",
        "best_selective_acc = selective_results[best_selective_n]['optimized_accuracy']\n",
        "\n",
        "print(f\"\\n🏆 STRATEGY 1 RESULTS:\")\n",
        "print(f\"Best selective hybrid: {best_selective_acc:.4f} with {best_selective_n} CNN features\")\n",
        "\n",
        "# ==========================================================\n",
        "# STRATEGY 2: WEIGHTED ENSEMBLE\n",
        "# ==========================================================\n",
        "print(f\"\\n🎯 STRATEGY 2: Weighted Ensemble\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get individual model predictions on test set\n",
        "print(\"Getting individual model predictions...\")\n",
        "\n",
        "# CNN predictions (your ensemble)\n",
        "cnn_test_preds = []\n",
        "for model in cnn_models:\n",
        "    pred = model.predict(X_test_grid, batch_size=64, verbose=0).ravel()\n",
        "    cnn_test_preds.append(pred)\n",
        "\n",
        "cnn_ensemble_pred = np.mean(np.stack(cnn_test_preds, axis=0), axis=0)\n",
        "\n",
        "# XGBoost predictions (retrain properly to avoid overfitting comparison)\n",
        "print(\"Training fair XGBoost for ensemble...\")\n",
        "fair_xgb = xgb.XGBClassifier(random_state=42)\n",
        "fair_xgb.fit(X_tab_train_selected, y_train)\n",
        "xgb_pred_proba = fair_xgb.predict_proba(X_tab_test_selected)[:, 1]\n",
        "\n",
        "print(f\"CNN ensemble pred range: {cnn_ensemble_pred.min():.3f} - {cnn_ensemble_pred.max():.3f}\")\n",
        "print(f\"XGBoost pred range: {xgb_pred_proba.min():.3f} - {xgb_pred_proba.max():.3f}\")\n",
        "\n",
        "# Test different weight combinations\n",
        "weight_combinations = [\n",
        "    (0.8, 0.2),  # Heavy XGBoost\n",
        "    (0.7, 0.3),  # Moderate XGBoost\n",
        "    (0.6, 0.4),  # Balanced\n",
        "    (0.5, 0.5),  # Equal\n",
        "]\n",
        "\n",
        "ensemble_results = {}\n",
        "\n",
        "print(\"\\n🧪 Testing weight combinations...\")\n",
        "\n",
        "for xgb_weight, cnn_weight in weight_combinations:\n",
        "    # Weighted ensemble\n",
        "    ensemble_pred = xgb_weight * xgb_pred_proba + cnn_weight * cnn_ensemble_pred\n",
        "\n",
        "    # Test different thresholds\n",
        "    thresholds = np.linspace(0.3, 0.7, 101)\n",
        "    best_acc = 0\n",
        "    best_thresh = 0.5\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        acc = accuracy_score(y_test, (ensemble_pred >= thresh).astype(int))\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_thresh = thresh\n",
        "\n",
        "    ensemble_results[(xgb_weight, cnn_weight)] = {\n",
        "        'accuracy': best_acc,\n",
        "        'threshold': best_thresh,\n",
        "        'predictions': ensemble_pred\n",
        "    }\n",
        "\n",
        "    print(f\"XGBoost {xgb_weight:.1f} + CNN {cnn_weight:.1f}: {best_acc:.4f} (thresh: {best_thresh:.3f})\")\n",
        "\n",
        "# Find best ensemble\n",
        "best_weights = max(ensemble_results.keys(), key=lambda k: ensemble_results[k]['accuracy'])\n",
        "best_ensemble_acc = ensemble_results[best_weights]['accuracy']\n",
        "best_ensemble_thresh = ensemble_results[best_weights]['threshold']\n",
        "\n",
        "print(f\"\\n🏆 STRATEGY 2 RESULTS:\")\n",
        "print(f\"Best ensemble: XGBoost {best_weights[0]:.1f} + CNN {best_weights[1]:.1f}\")\n",
        "print(f\"Best accuracy: {best_ensemble_acc:.4f} (threshold: {best_ensemble_thresh:.3f})\")\n",
        "\n",
        "# ==========================================================\n",
        "# COMPARE ALL APPROACHES\n",
        "# ==========================================================\n",
        "print(f\"\\n📊 COMPREHENSIVE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Individual model accuracies (recalculate fairly)\n",
        "cnn_individual_acc = accuracy_score(y_test, (cnn_ensemble_pred >= 0.5).astype(int))\n",
        "xgb_individual_acc = accuracy_score(y_test, (xgb_pred_proba >= 0.5).astype(int))\n",
        "\n",
        "print(f\"Individual Models:\")\n",
        "print(f\"   CNN Ensemble:           {cnn_individual_acc:.4f}\")\n",
        "print(f\"   XGBoost (fair):         {xgb_individual_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nOptimized Approaches:\")\n",
        "print(f\"   🎯 Selective Hybrid:    {best_selective_acc:.4f}\")\n",
        "print(f\"   🎯 Weighted Ensemble:   {best_ensemble_acc:.4f}\")\n",
        "\n",
        "# Find the overall best approach\n",
        "all_results = {\n",
        "    'CNN': cnn_individual_acc,\n",
        "    'XGBoost': xgb_individual_acc,\n",
        "    'Selective Hybrid': best_selective_acc,\n",
        "    'Weighted Ensemble': best_ensemble_acc\n",
        "}\n",
        "\n",
        "best_approach = max(all_results.keys(), key=lambda k: all_results[k])\n",
        "best_overall_acc = all_results[best_approach]\n",
        "\n",
        "print(f\"\\n🏆 BEST OVERALL APPROACH: {best_approach}\")\n",
        "print(f\"🎯 Best Accuracy: {best_overall_acc:.4f}\")\n",
        "\n",
        "if best_overall_acc > 0.99:\n",
        "    print(f\"\\n🎉 SUCCESS! Achieved {best_overall_acc:.4f} (>99%)!\")\n",
        "    success_margin = best_overall_acc - 0.99\n",
        "    print(f\"   Exceeded 99% by +{success_margin:.4f}!\")\n",
        "elif best_overall_acc > 0.995:\n",
        "    print(f\"\\n🔥 SO CLOSE! {best_overall_acc:.4f}\")\n",
        "    remaining = 0.99 - best_overall_acc\n",
        "    print(f\"   Only need +{remaining:.4f} more!\")\n",
        "else:\n",
        "    remaining = 0.99 - best_overall_acc\n",
        "    print(f\"\\n📈 Good progress: {best_overall_acc:.4f}\")\n",
        "    print(f\"   Need +{remaining:.4f} to reach 99%\")\n",
        "\n",
        "# ==========================================================\n",
        "# DETAILED ANALYSIS OF BEST MODEL\n",
        "# ==========================================================\n",
        "print(f\"\\n🔍 DETAILED ANALYSIS OF BEST MODEL ({best_approach})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if best_approach == 'Selective Hybrid':\n",
        "    best_model_pred = selective_results[best_selective_n]['predictions']\n",
        "    best_threshold = selective_results[best_selective_n]['best_threshold']\n",
        "    final_pred = (best_model_pred >= best_threshold).astype(int)\n",
        "\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"   - Tabular features: {len(selected_features)}\")\n",
        "    print(f\"   - CNN features: {best_selective_n}\")\n",
        "    print(f\"   - Total features: {len(selected_features) + best_selective_n}\")\n",
        "\n",
        "elif best_approach == 'Weighted Ensemble':\n",
        "    best_model_pred = ensemble_results[best_weights]['predictions']\n",
        "    best_threshold = ensemble_results[best_weights]['threshold']\n",
        "    final_pred = (best_model_pred >= best_threshold).astype(int)\n",
        "\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"   - XGBoost weight: {best_weights[0]:.1f}\")\n",
        "    print(f\"   - CNN weight: {best_weights[1]:.1f}\")\n",
        "    print(f\"   - Threshold: {best_threshold:.3f}\")\n",
        "\n",
        "else:\n",
        "    # Individual model\n",
        "    if best_approach == 'XGBoost':\n",
        "        final_pred = (xgb_pred_proba >= 0.5).astype(int)\n",
        "    else:  # CNN\n",
        "        final_pred = (cnn_ensemble_pred >= 0.5).astype(int)\n",
        "\n",
        "# Calculate detailed metrics for best model\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_test, final_pred, average='binary')\n",
        "\n",
        "print(f\"\\nDetailed Metrics:\")\n",
        "print(f\"   Accuracy:  {best_overall_acc:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "if best_approach in ['Selective Hybrid', 'Weighted Ensemble']:\n",
        "    auc = roc_auc_score(y_test, best_model_pred)\n",
        "    print(f\"   AUC:       {auc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, final_pred, digits=4))\n",
        "\n",
        "# ==========================================================\n",
        "# SAVE BEST MODEL\n",
        "# ==========================================================\n",
        "print(f\"\\n💾 Saving best model ({best_approach})...\")\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/Colab Notebooks/hybrid_models/best_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "if best_approach == 'Selective Hybrid':\n",
        "    best_model = selective_results[best_selective_n]['model']\n",
        "    model_path = os.path.join(save_dir, f\"best_selective_hybrid_{best_selective_n}cnn.pkl\")\n",
        "\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'model': best_model,\n",
        "            'approach': 'selective_hybrid',\n",
        "            'tabular_features': selected_features,\n",
        "            'cnn_feature_count': best_selective_n,\n",
        "            'cnn_feature_indices': selected_indices,\n",
        "            'best_threshold': best_threshold,\n",
        "            'performance': {\n",
        "                'accuracy': best_overall_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            }\n",
        "        }, f)\n",
        "\n",
        "elif best_approach == 'Weighted Ensemble':\n",
        "    model_path = os.path.join(save_dir, f\"best_weighted_ensemble.pkl\")\n",
        "\n",
        "    with open(model_path, 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'approach': 'weighted_ensemble',\n",
        "            'xgb_model': fair_xgb,\n",
        "            'tabular_features': selected_features,\n",
        "            'weights': {'xgb': best_weights[0], 'cnn': best_weights[1]},\n",
        "            'best_threshold': best_threshold,\n",
        "            'performance': {\n",
        "                'accuracy': best_overall_acc,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1\n",
        "            },\n",
        "            'note': 'Requires CNN models for CNN predictions'\n",
        "        }, f)\n",
        "\n",
        "print(f\"✅ Best model saved: {os.path.basename(model_path)}\")\n",
        "\n",
        "# ==========================================================\n",
        "# FINAL RECOMMENDATIONS\n",
        "# ==========================================================\n",
        "print(f\"\\n💡 FINAL RECOMMENDATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if best_overall_acc > 0.99:\n",
        "    print(f\"🎉 MISSION ACCOMPLISHED!\")\n",
        "    print(f\"   - Achieved: {best_overall_acc:.4f}\")\n",
        "    print(f\"   - Method: {best_approach}\")\n",
        "    print(f\"   - Ready for deployment!\")\n",
        "\n",
        "elif best_overall_acc > 0.995:\n",
        "    print(f\"🔥 VERY CLOSE TO 99%!\")\n",
        "    print(f\"   - Current: {best_overall_acc:.4f}\")\n",
        "    print(f\"   - Only need: +{0.99 - best_overall_acc:.4f}\")\n",
        "    print(f\"\\n   Next steps to reach 99%:\")\n",
        "    print(f\"   • Fine-tune hyperparameters of best approach\")\n",
        "    print(f\"   • Try ensemble of multiple approaches\")\n",
        "    print(f\"   • Add feature engineering\")\n",
        "\n",
        "else:\n",
        "    improvement_needed = 0.99 - best_overall_acc\n",
        "    print(f\"📈 GOOD PROGRESS!\")\n",
        "    print(f\"   - Current best: {best_overall_acc:.4f}\")\n",
        "    print(f\"   - Need: +{improvement_needed:.4f} for 99%\")\n",
        "    print(f\"\\n   Recommended next steps:\")\n",
        "    print(f\"   • Advanced hyperparameter optimization\")\n",
        "    print(f\"   • Data augmentation techniques\")\n",
        "    print(f\"   • Different CNN architectures\")\n",
        "\n",
        "print(f\"\\n📋 SUMMARY:\")\n",
        "print(f\"✅ Strategy 1 (Selective): {best_selective_acc:.4f}\")\n",
        "print(f\"✅ Strategy 2 (Ensemble):  {best_ensemble_acc:.4f}\")\n",
        "print(f\"🏆 Best overall:            {best_overall_acc:.4f}\")\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print(f\"\\n🎉 STRATEGIES 1 & 2 COMPLETED!\")\n",
        "print(f\"Ready to proceed with deployment or further optimization! 🚀\")"
      ],
      "metadata": {
        "id": "hGx343E7HR61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL COMPARISON AMONG MEMBERS"
      ],
      "metadata": {
        "id": "700IfuYTHsuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Paths to the metadata/results files\n",
        "gcn_metadata_path = \"/content/drive/MyDrive/Colab Notebooks/gcn_models/gcn_metadata.pkl\" # Assuming you saved GCN metadata similarly\n",
        "lgbm_metadata_path = \"/content/drive/MyDrive/DLI/lightgbm_top32.pkl\" # LGBM saved the model directly\n",
        "xgb_metadata_path = \"/content/drive/MyDrive/Colab Notebooks/xgb_models/xgb_metadata.pkl\" # XGBoost metadata\n",
        "\n",
        "# Initialize a list to store results\n",
        "results_list = []\n",
        "\n",
        "# --- Get GCN results ---\n",
        "try:\n",
        "    # Need to re-calculate GCN metrics from the test mask if metadata doesn't store them\n",
        "    # For now, we'll assume we can get them from the last run or need to re-evaluate\n",
        "    # As we don't have a saved GCN metadata with final test metrics, we'll use the printed output\n",
        "    # A more robust approach would be to save GCN metrics in its training script\n",
        "    # Placeholder - assuming the last executed cell for GCN print the test metrics\n",
        "    print(\"Please manually look at the GCN test metrics printed above.\")\n",
        "    print(\"If you need to capture GCN metrics programmatically, you would need to modify the GCN training cell to save them.\")\n",
        "\n",
        "    # Placeholder for GCN metrics - replace with actual loading if saved\n",
        "    gcn_acc = None\n",
        "    gcn_prec = None\n",
        "    gcn_rec = None\n",
        "    gcn_f1 = None\n",
        "\n",
        "    # If you modified the GCN cell to save metadata:\n",
        "    # with open(gcn_metadata_path, 'rb') as f:\n",
        "    #     gcn_metadata = pickle.load(f)\n",
        "    # gcn_perf = gcn_metadata['performance']\n",
        "    # gcn_acc = gcn_perf.get('test_accuracy')\n",
        "    # gcn_prec = gcn_perf.get('test_precision')\n",
        "    # gcn_rec = gcn_perf.get('test_recall')\n",
        "    # gcn_f1 = gcn_perf.get('test_f1')\n",
        "\n",
        "\n",
        "    # Add GCN results if available (replace None with actual values)\n",
        "    if gcn_acc is not None:\n",
        "         results_list.append({\n",
        "            \"Model\": \"GCN\",\n",
        "            \"Accuracy\": gcn_acc,\n",
        "            \"Precision\": gcn_prec,\n",
        "            \"Recall\": gcn_rec,\n",
        "            \"F1 Score\": gcn_f1,\n",
        "            \"Approach\": \"Graph Neural Network\"\n",
        "        })\n",
        "    else:\n",
        "         results_list.append({\n",
        "            \"Model\": \"GCN\",\n",
        "            \"Accuracy\": \"N/A (See output above)\",\n",
        "            \"Precision\": \"N/A\",\n",
        "            \"Recall\": \"N/A\",\n",
        "            \"F1 Score\": \"N/A\",\n",
        "            \"Approach\": \"Graph Neural Network\"\n",
        "        })\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load GCN metadata or metrics: {e}\")\n",
        "    results_list.append({\n",
        "        \"Model\": \"GCN\",\n",
        "        \"Accuracy\": \"Error Loading\",\n",
        "        \"Precision\": \"Error Loading\",\n",
        "        \"Recall\": \"Error Loading\",\n",
        "        \"F1 Score\": \"Error Loading\",\n",
        "        \"Approach\": \"Graph Neural Network\"\n",
        "    })\n",
        "\n",
        "\n",
        "# --- Get LightGBM results ---\n",
        "try:\n",
        "    # LGBM saved the model directly, the metrics were printed.\n",
        "    # We need to re-run the LGBM cell or add saving logic there to get metrics programmatically.\n",
        "    # For now, we'll extract from the printed output.\n",
        "    print(\"\\nPlease manually look at the LightGBM test metrics printed above.\")\n",
        "\n",
        "    # Placeholder for LGBM metrics - replace with actual loading if saved\n",
        "    lgbm_acc = None\n",
        "    lgbm_prec = None\n",
        "    lgbm_rec = None\n",
        "    lgbm_f1 = None\n",
        "\n",
        "    # If you modified the LGBM cell to save metadata:\n",
        "    # with open(lgbm_metadata_path, 'rb') as f:\n",
        "    #     lgbm_bundle = pickle.load(f)\n",
        "    # lgbm_perf = lgbm_bundle['performance'] # Assuming it's stored in a 'performance' key if saved differently\n",
        "    # lgbm_acc = lgbm_perf.get('Accuracy') # Or whatever keys were used\n",
        "    # lgbm_prec = lgbm_perf.get('Precision')\n",
        "    # lgbm_rec = lgbm_perf.get('Recall')\n",
        "    # lgbm_f1 = lgbm_perf.get('F1 Score')\n",
        "\n",
        "    # Add LGBM results if available\n",
        "    if lgbm_acc is not None:\n",
        "        results_list.append({\n",
        "            \"Model\": \"LightGBM\",\n",
        "            \"Accuracy\": lgbm_acc,\n",
        "            \"Precision\": lgbm_prec,\n",
        "            \"Recall\": lgbm_rec,\n",
        "            \"F1 Score\": lgbm_f1,\n",
        "            \"Approach\": \"Gradient Boosting (Tabular)\"\n",
        "        })\n",
        "    else:\n",
        "         results_list.append({\n",
        "            \"Model\": \"LightGBM\",\n",
        "            \"Accuracy\": \"N/A (See output above)\",\n",
        "            \"Precision\": \"N/A\",\n",
        "            \"Recall\": \"N/A\",\n",
        "            \"F1 Score\": \"N/A\",\n",
        "            \"Approach\": \"Gradient Boosting (Tabular)\"\n",
        "        })\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load LightGBM metrics: {e}\")\n",
        "    results_list.append({\n",
        "        \"Model\": \"LightGBM\",\n",
        "        \"Accuracy\": \"Error Loading\",\n",
        "        \"Precision\": \"Error Loading\",\n",
        "        \"Recall\": \"Error Loading\",\n",
        "        \"F1 Score\": \"Error Loading\",\n",
        "        \"Approach\": \"Gradient Boosting (Tabular)\"\n",
        "    })\n",
        "\n",
        "# --- Get CNN Ensemble results ---\n",
        "try:\n",
        "    cnn_metadata_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/cnn_models\", \"cnn_metadata.pkl\")\n",
        "    with open(cnn_metadata_path, 'rb') as f:\n",
        "        cnn_metadata = pickle.load(f)\n",
        "    cnn_perf = cnn_metadata['performance']\n",
        "\n",
        "    results_list.append({\n",
        "        \"Model\": \"CNN Ensemble\",\n",
        "        \"Accuracy\": cnn_perf.get('test_accuracy'),\n",
        "        \"Precision\": cnn_perf.get('test_precision'),\n",
        "        \"Recall\": cnn_perf.get('test_recall'),\n",
        "        \"F1 Score\": cnn_perf.get('test_f1'),\n",
        "        \"Approach\": \"Convolutional Neural Network (Grid)\"\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(f\"Could not load CNN Ensemble metadata: {e}\")\n",
        "    results_list.append({\n",
        "        \"Model\": \"CNN Ensemble\",\n",
        "        \"Accuracy\": \"Error Loading\",\n",
        "        \"Precision\": \"Error Loading\",\n",
        "        \"Recall\": \"Error Loading\",\n",
        "        \"F1 Score\": \"Error Loading\",\n",
        "        \"Approach\": \"Convolutional Neural Network (Grid)\"\n",
        "    })\n",
        "\n",
        "\n",
        "# --- Get Hybrid Model results ---\n",
        "try:\n",
        "    # Load the metadata for the *best* hybrid model\n",
        "    hybrid_metadata_dir = \"/content/drive/MyDrive/Colab Notebooks/hybrid_models/best_models\"\n",
        "    hybrid_pkl_files = [f for f in os.listdir(hybrid_metadata_dir) if f.endswith('.pkl')]\n",
        "\n",
        "    if hybrid_pkl_files:\n",
        "        # Assuming there's only one best model saved or we pick the first one\n",
        "        hybrid_metadata_path = os.path.join(hybrid_metadata_dir, hybrid_pkl_files[0])\n",
        "        with open(hybrid_metadata_path, 'rb') as f:\n",
        "            hybrid_metadata = pickle.load(f)\n",
        "\n",
        "        hybrid_perf = hybrid_metadata['performance']\n",
        "        hybrid_approach = hybrid_metadata.get('approach', 'Unknown Hybrid')\n",
        "\n",
        "        results_list.append({\n",
        "            \"Model\": f\"Hybrid ({hybrid_approach})\",\n",
        "            \"Accuracy\": hybrid_perf.get('accuracy'),\n",
        "            \"Precision\": hybrid_perf.get('precision'),\n",
        "            \"Recall\": hybrid_perf.get('recall'),\n",
        "            \"F1 Score\": hybrid_perf.get('f1'),\n",
        "            \"Approach\": hybrid_approach.replace('_', ' ').title() # Make it readable\n",
        "        })\n",
        "    else:\n",
        "         print(\"No best hybrid model found. Run the Hybrid cell first.\")\n",
        "         results_list.append({\n",
        "            \"Model\": \"Hybrid\",\n",
        "            \"Accuracy\": \"N/A\",\n",
        "            \"Precision\": \"N/A\",\n",
        "            \"Recall\": \"N/A\",\n",
        "            \"F1 Score\": \"N/A\",\n",
        "            \"Approach\": \"Hybrid (Selective/Ensemble)\"\n",
        "        })\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load Hybrid model metadata: {e}\")\n",
        "    results_list.append({\n",
        "        \"Model\": \"Hybrid\",\n",
        "        \"Accuracy\": \"Error Loading\",\n",
        "        \"Precision\": \"Error Loading\",\n",
        "        \"Recall\": \"Error Loading\",\n",
        "        \"F1 Score\": \"Error Loading\",\n",
        "        \"Approach\": \"Hybrid (Selective/Ensemble)\"\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# Create DataFrame\n",
        "comparison_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Set Model as index for better display\n",
        "comparison_df = comparison_df.set_index(\"Model\")\n",
        "\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Optional: Highlight the best metric for each column\n",
        "def highlight_max(s):\n",
        "    if s.dtype == object: # Handle N/A or Error values\n",
        "        return ['' for _ in s]\n",
        "    is_max = s == s.max()\n",
        "    return ['background-color: yellow' if v else '' for v in is_max]\n",
        "\n",
        "# Display with highlighting, excluding 'Approach' column\n",
        "display(comparison_df.style.apply(highlight_max, subset=['Accuracy', 'Precision', 'Recall', 'F1 Score'], axis=0))"
      ],
      "metadata": {
        "id": "jEUWb3E7Hw4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL GROUP MODEL"
      ],
      "metadata": {
        "id": "zQCWB_8mHx5b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptzRbAyOHzf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
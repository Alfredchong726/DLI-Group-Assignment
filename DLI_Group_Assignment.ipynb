{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## First commint to GitHub"
      ],
      "metadata": {
        "id": "eefkLYk01aJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf25uYCu8Orn",
        "outputId": "0f560279-a96c-4753-b35e-e2e4f1a19d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  DATA CLEANING & PREPROCESSING\n",
        "\n"
      ],
      "metadata": {
        "id": "AIYfbYQlprBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up & Imports"
      ],
      "metadata": {
        "id": "mqzqKc5TCHuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict\n",
        "import sys, json, glob, re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Versions -> pandas:\", pd.__version__, \"| numpy:\", np.__version__)"
      ],
      "metadata": {
        "id": "Jd4UXRmzpwuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config"
      ],
      "metadata": {
        "id": "aVaC2byzCl48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_DRIVE = True                            # Mount Google Drive to search for the file\n",
        "DRIVE_SEARCH_DIR = \"/content/drive/MyDrive\" # Root folder to search\n",
        "FILE_PATTERN = \"**/Phishing_Mendeley*.csv\"  # Pattern to locate your CSV in Drive\n",
        "FALLBACK_PROMPT_UPLOAD = True               # If not found, open an upload dialog\n",
        "\n",
        "# Behaviors\n",
        "DROP_DUPLICATES = True                      # Only drop full-row duplicates (incl. 'id'); if id differs -> keep\n",
        "DROP_HIGH_MISSING_COLS = False              # If True, drop cols with missing rate > HIGH_MISSING_THRESHOLD\n",
        "HIGH_MISSING_THRESHOLD = 0.40\n",
        "\n",
        "# Save to Drive too?\n",
        "SAVE_BACK_TO_DRIVE = False                  # If True, also copy outputs to DRIVE_OUT_DIR\n",
        "DRIVE_OUT_DIR = \"/content/drive/MyDrive/phishing_cleaned_outputs\""
      ],
      "metadata": {
        "id": "x5hF-nyiCmVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive/Load Utilities"
      ],
      "metadata": {
        "id": "-rKW50bkCn2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive_if_needed():\n",
        "    if USE_DRIVE:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Drive mounted.\")\n",
        "        except Exception as e:\n",
        "            print(\"Drive mount failed or not in Colab:\", e)\n",
        "\n",
        "def find_csv_in_drive(search_dir: str, pattern: str) -> Optional[str]:\n",
        "    paths = glob.glob(str(Path(search_dir) / pattern), recursive=True)\n",
        "    return max(paths, key=lambda p: Path(p).stat().st_mtime) if paths else None\n",
        "\n",
        "def upload_dialog() -> Optional[str]:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Please upload your CSV fileâ€¦\")\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            return None\n",
        "        name = next(iter(uploaded.keys()))\n",
        "        print(\"Uploaded:\", name)\n",
        "        return str(Path(\"/content\") / name)\n",
        "    except Exception as e:\n",
        "        print(\"Upload dialog not available (not in Colab?):\", e)\n",
        "        return None\n",
        "\n",
        "def read_raw(p: str | Path) -> pd.DataFrame:\n",
        "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
        "        try:\n",
        "            return pd.read_csv(p, encoding=enc, engine=\"python\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    raise RuntimeError(\"Failed to read CSV with utf-8 / utf-8-sig / latin-1.\")"
      ],
      "metadata": {
        "id": "9FfiynjkCq_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Locate & Load the CSV"
      ],
      "metadata": {
        "id": "CRti-H5YCtFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mount_drive_if_needed()\n",
        "\n",
        "csv_path = None\n",
        "if USE_DRIVE:\n",
        "    csv_path = find_csv_in_drive(DRIVE_SEARCH_DIR, FILE_PATTERN)\n",
        "    print(\"Drive search:\", \"FOUND\" if csv_path else \"Not found\")\n",
        "\n",
        "if not csv_path and FALLBACK_PROMPT_UPLOAD:\n",
        "    csv_path = upload_dialog()\n",
        "\n",
        "if not csv_path:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not locate a dataset. Set USE_DRIVE=True with correct DRIVE_SEARCH_DIR/FILE_PATTERN \"\n",
        "        \"or enable FALLBACK_PROMPT_UPLOAD.\"\n",
        "    )\n",
        "\n",
        "print(\"Using dataset:\", csv_path)\n",
        "\n",
        "raw_df = read_raw(csv_path)\n",
        "orig_shape = (int(raw_df.shape[0]), int(raw_df.shape[1]))\n",
        "\n",
        "# Preserve CamelCase names; trim whitespace inside string cells\n",
        "df = raw_df.copy()\n",
        "for c in df.select_dtypes(include=[object]).columns:\n",
        "    df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "print(\"Loaded shape:\", df.shape)"
      ],
      "metadata": {
        "id": "ntWEv5kaCtu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info"
      ],
      "metadata": {
        "id": "axdgtcpOCu2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from textwrap import indent\n",
        "\n",
        "print(\"\\n=== DATASET INFO ===\")\n",
        "print(\"Path:\", csv_path)\n",
        "print(\"Original shape:\", orig_shape)\n",
        "\n",
        "print(\"\\n.dtypes (first 50):\")\n",
        "print(df.dtypes.head(50))\n",
        "\n",
        "print(\"\\n.info():\")\n",
        "# Capture df.info() into a string buffer so we can print it nicely\n",
        "buffer = io.StringIO()\n",
        "df.info(buf=buffer)\n",
        "info_str = buffer.getvalue()\n",
        "print(info_str)\n",
        "\n",
        "print(\"\\n.head(5):\")\n",
        "display(df.head(5))\n",
        "\n",
        "print(\"\\nMissingness (top 20):\")\n",
        "miss = df.isna().mean().sort_values(ascending=False)\n",
        "display(miss.head(20).to_frame(\"missing_rate\"))\n",
        "\n",
        "# Early guess of target column (just for info; final alignment happens later)\n",
        "target_guess = next(\n",
        "    (c for c in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]\n",
        "     if c in df.columns),\n",
        "    None\n",
        ")\n",
        "print(\"\\nTarget column guess:\", target_guess)\n",
        "if target_guess is not None:\n",
        "    # Show a small sample of unique values\n",
        "    try:\n",
        "        uniques = pd.unique(df[target_guess].dropna())\n",
        "        print(\"Sample target values:\", uniques[:10])\n",
        "    except Exception as e:\n",
        "        print(\"Could not preview target values:\", e)"
      ],
      "metadata": {
        "id": "Z6WPc6GSCx7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions for Cleaning"
      ],
      "metadata": {
        "id": "rZbpPFIXCzPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coerce_numeric_like(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Coerce object columns that look numeric (>=80% numeric-like) into numeric dtype.\"\"\"\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            s = df[c].astype(str).str.strip()\n",
        "            mask = s.str.match(r'^[+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?$')\n",
        "            if mask.mean() >= 0.8:\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def handle_infinities(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def drop_low_variance(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, list]:\n",
        "    nunique = df.nunique(dropna=False)\n",
        "    lowvar = nunique[nunique <= 1].index.tolist()\n",
        "    if target_col in lowvar:\n",
        "        lowvar.remove(target_col)\n",
        "    if lowvar:\n",
        "        df = df.drop(columns=lowvar)\n",
        "    return df, lowvar\n",
        "\n",
        "def impute_missing(df: pd.DataFrame, target_col: Optional[str]) -> Tuple[pd.DataFrame, Dict]:\n",
        "    df = df.copy()\n",
        "    report: Dict = {}\n",
        "\n",
        "    if DROP_HIGH_MISSING_COLS:\n",
        "        miss_rate = df.isna().mean().sort_values(ascending=False)\n",
        "        drop_cols = miss_rate[miss_rate > HIGH_MISSING_THRESHOLD].index.tolist()\n",
        "        if target_col in drop_cols:\n",
        "            drop_cols.remove(target_col)\n",
        "        if drop_cols:\n",
        "            df = df.drop(columns=drop_cols)\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": drop_cols\n",
        "        }\n",
        "    else:\n",
        "        report[\"dropped_columns_missing_gt_threshold\"] = {\n",
        "            \"threshold\": HIGH_MISSING_THRESHOLD,\n",
        "            \"columns\": []\n",
        "        }\n",
        "\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
        "    if target_col and target_col in cat_cols:\n",
        "        cat_cols.remove(target_col)\n",
        "\n",
        "    imputations = {\"numeric\": {}, \"categorical\": {}}\n",
        "    for c in num_cols:\n",
        "        if df[c].isna().any():\n",
        "            med = df[c].median()\n",
        "            df[c] = df[c].fillna(med)\n",
        "            imputations[\"numeric\"][c] = None if pd.isna(med) else float(med)\n",
        "\n",
        "    for c in cat_cols:\n",
        "        if df[c].isna().any():\n",
        "            mode = df[c].mode(dropna=True)\n",
        "            val = mode.iloc[0] if not mode.empty else \"__missing__\"\n",
        "            df[c] = df[c].fillna(val)\n",
        "            imputations[\"categorical\"][c] = val\n",
        "\n",
        "    report[\"imputations\"] = imputations\n",
        "    return df, report\n",
        "\n",
        "def finalize_int_casts(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if pd.api.types.is_float_dtype(df[c]):\n",
        "            s = df[c]\n",
        "            if np.allclose(s.dropna() % 1, 0):\n",
        "                try:\n",
        "                    df[c] = s.astype(\"Int64\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "    return df\n",
        "\n",
        "def make_jsonable(obj):\n",
        "    import numpy as _np\n",
        "    import pandas as _pd\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_jsonable(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple, set)):\n",
        "        return [make_jsonable(v) for v in obj]\n",
        "    if isinstance(obj, (_np.integer,)):\n",
        "        return int(obj)\n",
        "    if isinstance(obj, (_np.floating,)):\n",
        "        return float(obj)\n",
        "    if isinstance(obj, (_np.bool_,)):\n",
        "        return bool(obj)\n",
        "    if isinstance(obj, _np.ndarray):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.Series):\n",
        "        return obj.tolist()\n",
        "    if isinstance(obj, _pd.DataFrame):\n",
        "        return obj.to_dict(orient=\"list\")\n",
        "    return obj"
      ],
      "metadata": {
        "id": "4AlaijahC1XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Pipeline"
      ],
      "metadata": {
        "id": "aO2jMAy3C2tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_df = df.copy()\n",
        "report = {\n",
        "    \"source\": csv_path,\n",
        "    \"original_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"settings\": {\n",
        "        \"DROP_DUPLICATES\": bool(DROP_DUPLICATES),\n",
        "        \"DROP_HIGH_MISSING_COLS\": bool(DROP_HIGH_MISSING_COLS),\n",
        "        \"HIGH_MISSING_THRESHOLD\": float(HIGH_MISSING_THRESHOLD),\n",
        "    },\n",
        "    \"steps\": {}\n",
        "}\n",
        "\n",
        "# Duplicates (full-row)\n",
        "dup_count = int(work_df.duplicated().sum())\n",
        "if DROP_DUPLICATES and dup_count > 0:\n",
        "    work_df = work_df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
        "report[\"steps\"][\"duplicate_rows_found_full_row\"] = dup_count\n",
        "report[\"steps\"][\"duplicates_removed\"] = int(dup_count if DROP_DUPLICATES else 0)\n",
        "print(f\"Duplicates found: {dup_count} | Removed: {report['steps']['duplicates_removed']}\")\n",
        "\n",
        "# Drop non-predictive ID AFTER dedupe\n",
        "dropped_non_predictive = []\n",
        "if \"id\" in work_df.columns:\n",
        "    work_df = work_df.drop(columns=[\"id\"])\n",
        "    dropped_non_predictive.append(\"id\")\n",
        "report[\"steps\"][\"dropped_non_predictive\"] = dropped_non_predictive\n",
        "if dropped_non_predictive:\n",
        "    print(\"Dropped columns (non-predictive):\", dropped_non_predictive)\n",
        "\n",
        "# Coerce numeric-like; handle Â±inf\n",
        "work_df = coerce_numeric_like(work_df)\n",
        "work_df = handle_infinities(work_df)\n",
        "\n",
        "# Target alignment (prefer CLASS_LABEL, but auto-detect if changed)\n",
        "target_col = None\n",
        "for cand in [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]:\n",
        "    if cand in work_df.columns:\n",
        "        target_col = cand\n",
        "        break\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found (expected 'CLASS_LABEL' or close variant).\")\n",
        "\n",
        "# Ensure numeric binary target\n",
        "if work_df[target_col].dtype == object:\n",
        "    y_num = pd.to_numeric(work_df[target_col], errors=\"coerce\")\n",
        "    if y_num.isna().any():\n",
        "        y_num = pd.Series(pd.factorize(work_df[target_col].astype(str).str.strip().str.lower())[0], index=work_df.index)\n",
        "    work_df[target_col] = y_num\n",
        "\n",
        "uniq = set(pd.unique(work_df[target_col].dropna()))\n",
        "if uniq.issubset({-1, 0, 1}) and uniq != {0, 1}:\n",
        "    # If dataset uses -1/1 or -1/0/1, map negatives to 1 (phishing) and non-negatives to 0\n",
        "    work_df[target_col] = work_df[target_col].map(lambda v: 1 if v < 0 else (0 if v > 0 else 0))\n",
        "\n",
        "report[\"steps\"][\"target_info\"] = {\n",
        "    \"name\": target_col,\n",
        "    \"unique_values_after_normalization\": sorted([int(x) for x in pd.unique(work_df[target_col].dropna())])\n",
        "}\n",
        "print(\"Target column:\", target_col)\n",
        "print(\"Target uniques (post-normalization):\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "\n",
        "# Drop truly constant columns (except target)\n",
        "work_df, lowvar_dropped = drop_low_variance(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"low_variance_dropped\"] = lowvar_dropped\n",
        "if lowvar_dropped:\n",
        "    print(\"Dropped low-variance cols:\", lowvar_dropped)\n",
        "\n",
        "# Impute missing values (no row dropping)\n",
        "work_df, mv_report = impute_missing(work_df, target_col=target_col)\n",
        "report[\"steps\"][\"missing_value_handling\"] = mv_report\n",
        "print(\"Imputation summary:\", json.dumps(mv_report, indent=2)[:1000], \"...\")\n",
        "\n",
        "# Cast floats-that-are-integers to Int64\n",
        "work_df = finalize_int_casts(work_df)\n",
        "\n",
        "print(\"\\nPost-clean shape:\", work_df.shape)"
      ],
      "metadata": {
        "id": "CBXKaKKwC4Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Cleaned Data & Report"
      ],
      "metadata": {
        "id": "_utgsuMEC5ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_dir = Path(csv_path).parent  # same folder as original file\n",
        "csv_out = orig_dir / \"phishing_mendeley_cleaned.csv\"\n",
        "json_report = orig_dir / \"phishing_mendeley_cleaned_report.json\"\n",
        "\n",
        "# Save cleaned CSV\n",
        "work_df.to_csv(csv_out, index=False)\n",
        "\n",
        "# Update & save JSON report\n",
        "report.update({\n",
        "    \"final_shape\": (int(work_df.shape[0]), int(work_df.shape[1])),\n",
        "    \"row_delta\": int(work_df.shape[0] - orig_shape[0]),\n",
        "    \"outputs\": {\n",
        "        \"csv\": str(csv_out),\n",
        "        \"json_report\": str(json_report),\n",
        "    }\n",
        "})\n",
        "\n",
        "with open(json_report, \"w\", encoding=\"utf-8\") as jf:\n",
        "    json.dump(make_jsonable(report), jf, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Saved CSV       :\", csv_out)\n",
        "print(\"Saved JSON      :\", json_report)\n",
        "\n",
        "# Optional copy back to Drive output folder\n",
        "if SAVE_BACK_TO_DRIVE and USE_DRIVE:\n",
        "    outdir = Path(DRIVE_OUT_DIR)\n",
        "    outdir.mkdir(parents=True, exist_ok=True)\n",
        "    dst_csv = outdir / csv_out.name\n",
        "    dst_json = outdir / json_report.name\n",
        "    _ = Path(dst_csv).write_bytes(Path(csv_out).read_bytes())\n",
        "    _ = Path(dst_json).write_bytes(Path(json_report).read_bytes())\n",
        "    print(\"Also copied to  :\", outdir)"
      ],
      "metadata": {
        "id": "9JbkO6PPC7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Summary & Target Distribution"
      ],
      "metadata": {
        "id": "LaWqFpWIC-Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"Source          :\", report['source'])\n",
        "print(\"Original shape  :\", report['original_shape'])\n",
        "print(\"Final shape     :\", report['final_shape'])\n",
        "print(\"Row delta       :\", report['row_delta'])\n",
        "print(\"Dup (full-row)  :\", report['steps']['duplicate_rows_found_full_row'],\n",
        "      \"| removed:\", report['steps']['duplicates_removed'])\n",
        "print(\"Dropped (non-predictive):\", report['steps']['dropped_non_predictive'])\n",
        "\n",
        "print(\"\\nTarget:\", report[\"steps\"][\"target_info\"][\"name\"])\n",
        "print(\"Target uniques :\", report[\"steps\"][\"target_info\"][\"unique_values_after_normalization\"])\n",
        "print(\"\\nTarget distribution:\")\n",
        "display(work_df[report[\"steps\"][\"target_info\"][\"name\"]].value_counts(dropna=False).to_frame(\"count\"))\n",
        "\n",
        "print(\"\\nPreview cleaned data:\")\n",
        "display(work_df.head(10))"
      ],
      "metadata": {
        "id": "Wzi7_2yfC90N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ MODEL TRAINING ============"
      ],
      "metadata": {
        "id": "2sJvFhP-8c_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHONG MUN SEONG (TP063440)"
      ],
      "metadata": {
        "id": "tS5HMHxv73VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & constant"
      ],
      "metadata": {
        "id": "xzh5TVWOEp-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, re, math, json, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "TOP_K = 20\n",
        "USE_THRESHOLD = False\n",
        "THRESH = 0.9935\n",
        "\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.35\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 100\n",
        "VAL_SPLIT = 0.15\n",
        "TEST_SPLIT = 0.15\n",
        "EARLY_STOP_PATIENCE = 12\n",
        "LR_SCHED_PATIENCE = 5\n",
        "LR_DECAY_FACTOR = 0.5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "wY7i46GsEooH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load cleaned csv file"
      ],
      "metadata": {
        "id": "pk6TC7j8EsNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = glob.glob(\"/content/drive/**/phishing_mendeley_cleaned.csv\", recursive=True)\n",
        "if not candidates:\n",
        "    candidates = glob.glob(\"/content/**/phishing_mendeley_cleaned.csv\", recursive=True)\n",
        "\n",
        "if not candidates:\n",
        "    candidates = glob.glob(\"/content/drive/**/Phishing_Mendeley*.csv\", recursive=True) + \\\n",
        "                 glob.glob(\"/content/**/Phishing_Mendeley*.csv\", recursive=True)\n",
        "\n",
        "if not candidates:\n",
        "    raise FileNotFoundError(\"Couldn't find 'phishing_mendeley_cleaned.csv'. Please set csv_path manually below.\")\n",
        "\n",
        "csv_path = max(candidates, key=lambda p: Path(p).stat().st_mtime)\n",
        "print(\"Using:\", csv_path)\n",
        "\n",
        "df = pd.read_csv(csv_path, engine=\"python\")\n",
        "print(df.shape, \"columns:\", len(df.columns))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "vqVzdp5fEurC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL preprocessing & structural feature"
      ],
      "metadata": {
        "id": "90miUtR-Ev6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_candidates = [\"CLASS_LABEL\", \"class_label\", \"Class\", \"Label\", \"Result\", \"label\", \"result\", \"target\", \"Target\"]\n",
        "target_col = next((c for c in target_candidates if c in df.columns), None)\n",
        "if target_col is None:\n",
        "    raise ValueError(\"Target column not found. Please rename your label to one of: \" + \", \".join(target_candidates))\n",
        "\n",
        "y = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(0).astype(int).values\n",
        "print(\"Target:\", target_col, \"unique:\", np.unique(y, return_counts=True))\n",
        "\n",
        "url_col_candidates = [\"url\",\"URL\",\"Url\",\"URl\",\"full_url\",\"FullURL\",\"Address\",\"address\",\"Domain\",\"domain\",\"Hostname\",\"hostname\"]\n",
        "url_col = next((c for c in url_col_candidates if c in df.columns and df[c].astype(str).str.contains(r'\\.|http', na=False).any()), None)\n",
        "print(\"URL column detected:\", url_col)"
      ],
      "metadata": {
        "id": "1JWEOObWExr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL semantic features & structural features"
      ],
      "metadata": {
        "id": "3WfZxCWoEzS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_scheme_www(s: str) -> str:\n",
        "    s = re.sub(r'^\\s*https?://', '', str(s).strip(), flags=re.I)\n",
        "    s = re.sub(r'^\\s*www\\.', '', s, flags=re.I)\n",
        "    return s\n",
        "\n",
        "def is_ip_domain(host: str) -> bool:\n",
        "    return bool(re.fullmatch(r'\\d{1,3}(?:\\.\\d{1,3}){3}', host))\n",
        "\n",
        "SHORTENERS = set(\"\"\"\n",
        "bit.ly goo.gl t.co ow.ly is.gd buff.ly tinyurl.com lnkd.in rebrand.ly cutt.ly\n",
        "t.ly s.id v.gd adf.ly chilp.it clck.ru fb.me youtu.be\n",
        "\"\"\".split())\n",
        "\n",
        "def parse_host_path(url_no_scheme: str):\n",
        "    parts = url_no_scheme.split('/', 1)\n",
        "    host = parts[0].split('?')[0]\n",
        "    path = parts[1] if len(parts) > 1 else \"\"\n",
        "    return host.lower(), path\n",
        "\n",
        "def tld_prune(host: str):\n",
        "    segs = [s for s in host.split('.') if s]\n",
        "    if len(segs) >= 2:\n",
        "        core = segs[:-1]\n",
        "    else:\n",
        "        core = segs\n",
        "    if core and core[0] == 'www':\n",
        "        core = core[1:]\n",
        "    return core\n",
        "\n",
        "def subdomain_count(host: str) -> int:\n",
        "    core = tld_prune(host)\n",
        "    return max(0, len(core) - 1)\n",
        "\n",
        "def has_double_slash_in_path(path: str) -> bool:\n",
        "    return '//' in path\n",
        "\n",
        "def is_shortener(host: str) -> bool:\n",
        "    return host in SHORTENERS\n",
        "\n",
        "CHARS = [chr(i) for i in range(32, 127)]\n",
        "char2idx = {ch:i for i,ch in enumerate(CHARS)}\n",
        "EMB_DIM = 16\n",
        "rng = np.random.RandomState(SEED)\n",
        "char_emb = rng.normal(loc=0.0, scale=1.0, size=(len(CHARS), EMB_DIM)).astype(np.float32)\n",
        "\n",
        "def url_semantic_vec(url: str) -> np.ndarray:\n",
        "    if not isinstance(url, str):\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    s = strip_scheme_www(url)\n",
        "    if not s:\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    vecs = []\n",
        "    for ch in s:\n",
        "        idx = char2idx.get(ch)\n",
        "        if idx is not None:\n",
        "            vecs.append(char_emb[idx])\n",
        "    if not vecs:\n",
        "        return np.zeros(EMB_DIM, dtype=np.float32)\n",
        "    return np.mean(np.vstack(vecs), axis=0).astype(np.float32)\n",
        "\n",
        "def url_struct_vec(url: str) -> np.ndarray:\n",
        "    if not isinstance(url, str):\n",
        "        return np.zeros(7, dtype=np.float32)\n",
        "    raw = url.strip()\n",
        "    url_noscheme = strip_scheme_www(raw)\n",
        "    host, path = parse_host_path(url_noscheme)\n",
        "\n",
        "    f_ip = 1.0 if is_ip_domain(host) else 0.0\n",
        "    f_len = float(len(raw))\n",
        "    f_short = 1.0 if is_shortener(host) else 0.0\n",
        "    f_at = 1.0 if '@' in raw else 0.0\n",
        "    f_doubleslash = 1.0 if has_double_slash_in_path(path) else 0.0\n",
        "    f_dash = 1.0 if '-' in host else 0.0\n",
        "    f_sub = float(subdomain_count(host))\n",
        "\n",
        "    return np.array([f_ip, f_len, f_short, f_at, f_doubleslash, f_dash, f_sub], dtype=np.float32)"
      ],
      "metadata": {
        "id": "LegOkgRIE1Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build feature matrix"
      ],
      "metadata": {
        "id": "YREtkx8SE3I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_semantic = url_col is not None\n",
        "\n",
        "if use_semantic:\n",
        "    urls = df[url_col].astype(str).fillna(\"\")\n",
        "    SEM = np.vstack([url_semantic_vec(u) for u in urls])\n",
        "    STR = np.vstack([url_struct_vec(u) for u in urls])\n",
        "    X = np.hstack([SEM, STR])   # 16 + 7 = 23 dims\n",
        "    feature_desc = f\"Using SEM(16) + STR(7) => {X.shape[1]} dims\"\n",
        "else:\n",
        "    possible_map = {\n",
        "        \"having_IP_Address\": None,\n",
        "        \"URL_Length\": None,\n",
        "        \"Shortining_Service\": None,\n",
        "        \"having_At_Symbol\": None,\n",
        "        \"double_slash_redirecting\": None,\n",
        "        \"Prefix_Suffix\": None,\n",
        "        \"having_Sub_Domain\": None\n",
        "    }\n",
        "    avail = [c for c in possible_map if c in df.columns]\n",
        "    if len(avail) >= 3:\n",
        "        X = df[avail].astype(float).values\n",
        "        feature_desc = f\"Structural-only from dataset columns: {avail}\"\n",
        "    else:\n",
        "        num_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n",
        "        X = df[num_cols].astype(float).values\n",
        "        feature_desc = f\"No URL text or standard structural columns detected; using numeric features: {len(num_cols)} cols\"\n",
        "\n",
        "print(feature_desc)\n",
        "print(\"X shape:\", X.shape)"
      ],
      "metadata": {
        "id": "8Qsw2z6cE3fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Val/Test split"
      ],
      "metadata": {
        "id": "rVy6O5KfE5zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_all = np.arange(len(y))\n",
        "X_train, X_tmp, y_train, y_tmp, idx_train, idx_tmp = train_test_split(\n",
        "    X, y, idx_all, test_size=VAL_SPLIT + TEST_SPLIT, random_state=SEED, stratify=y)\n",
        "\n",
        "val_size = VAL_SPLIT / (VAL_SPLIT + TEST_SPLIT)\n",
        "X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
        "    X_tmp, y_tmp, idx_tmp, test_size=(1 - val_size), random_state=SEED, stratify=y_tmp)\n",
        "\n",
        "print(\"Splits:\", len(idx_train), len(idx_val), len(idx_test))\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_std = scaler.transform(X)\n",
        "\n",
        "train_mask = np.zeros(len(y), dtype=bool); train_mask[idx_train] = True\n",
        "val_mask   = np.zeros(len(y), dtype=bool); val_mask[idx_val]   = True\n",
        "test_mask  = np.zeros(len(y), dtype=bool); test_mask[idx_test] = True"
      ],
      "metadata": {
        "id": "KLGGbemSE6Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Top-k cosine graph"
      ],
      "metadata": {
        "id": "crBk4JJdE7pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_topk_graph(features: np.ndarray, k: int = 12, use_threshold=False, thresh=0.9935):\n",
        "    if use_threshold:\n",
        "        nn = NearestNeighbors(n_neighbors=min(64, features.shape[0]-1), metric='cosine', algorithm='auto', n_jobs=-1)\n",
        "        nn.fit(features)\n",
        "        dists, nbrs = nn.kneighbors(features, return_distance=True)\n",
        "        sims = 1.0 - dists\n",
        "        rows, cols = [], []\n",
        "        for i in range(features.shape[0]):\n",
        "            for sim, j in zip(sims[i], nbrs[i]):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if sim >= thresh:\n",
        "                    rows.append(i); cols.append(j)\n",
        "    else:\n",
        "        nn = NearestNeighbors(n_neighbors=min(k+1, features.shape[0]), metric='cosine', algorithm='auto', n_jobs=-1)\n",
        "        nn.fit(features)\n",
        "        dists, nbrs = nn.kneighbors(features, return_distance=True)\n",
        "        rows, cols = [], []\n",
        "        for i in range(features.shape[0]):\n",
        "            for j in nbrs[i]:\n",
        "                if i == j:\n",
        "                    continue\n",
        "                rows.append(i); cols.append(j)\n",
        "\n",
        "    edges = set(zip(rows, cols))\n",
        "    edges |= set((j,i) for (i,j) in edges)\n",
        "    rows, cols = zip(*edges) if edges else ([],[])\n",
        "    return np.array(rows, dtype=np.int64), np.array(cols, dtype=np.int64)\n",
        "\n",
        "rows, cols = build_topk_graph(X_std, k=TOP_K, use_threshold=USE_THRESHOLD, thresh=THRESH)\n",
        "print(\"Edges (undirected, no self-loops):\", len(rows))"
      ],
      "metadata": {
        "id": "Q9DF1QWzE82B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize adjacency and define a 2 layer GCN"
      ],
      "metadata": {
        "id": "tRegmx11E-Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_normalized_adj(n_nodes: int, rows: np.ndarray, cols: np.ndarray):\n",
        "    rows_all = np.concatenate([rows, np.arange(n_nodes)])\n",
        "    cols_all = np.concatenate([cols, np.arange(n_nodes)])\n",
        "    data = np.ones_like(rows_all, dtype=np.float32)\n",
        "\n",
        "    idx = np.vstack([rows_all, cols_all])\n",
        "    A = torch.sparse_coo_tensor(indices=idx, values=torch.from_numpy(data), size=(n_nodes, n_nodes))\n",
        "    A = A.coalesce()\n",
        "\n",
        "    deg = torch.sparse.sum(A, dim=1).to_dense()\n",
        "    deg_inv_sqrt = torch.pow(deg + 1e-8, -0.5)\n",
        "    d_i = deg_inv_sqrt[rows_all]\n",
        "    d_j = deg_inv_sqrt[cols_all]\n",
        "    norm_vals = torch.from_numpy(data) * d_i * d_j\n",
        "\n",
        "    A_norm = torch.sparse_coo_tensor(indices=idx, values=norm_vals, size=(n_nodes, n_nodes))\n",
        "    return A_norm.coalesce()\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, A_norm):\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sparse.mm(A_norm, x)\n",
        "        x = self.lin(x)\n",
        "        return x\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim=2, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.gcn1 = GCNLayer(in_dim, hidden, dropout)\n",
        "        self.gcn2 = GCNLayer(hidden, out_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, A_norm):\n",
        "        x = self.gcn1(x, A_norm)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gcn2(x, A_norm)\n",
        "        return x\n",
        "\n",
        "X_tensor = torch.from_numpy(X_std.astype(np.float32))\n",
        "y_tensor = torch.from_numpy(y.astype(np.int64))\n",
        "A_norm = build_normalized_adj(n_nodes=X_tensor.shape[0], rows=rows, cols=cols)\n",
        "\n",
        "train_mask_t = torch.from_numpy(train_mask)\n",
        "val_mask_t   = torch.from_numpy(val_mask)\n",
        "test_mask_t  = torch.from_numpy(test_mask)\n",
        "\n",
        "X_tensor = X_tensor.to(device)\n",
        "y_tensor = y_tensor.to(device)\n",
        "A_norm   = A_norm.to(device)\n",
        "train_mask_t = train_mask_t.to(device)\n",
        "val_mask_t = val_mask_t.to(device)\n",
        "test_mask_t = test_mask_t.to(device)\n",
        "\n",
        "model = GCN(in_dim=X_tensor.shape[1], hidden=HIDDEN_DIM, out_dim=2, dropout=DROPOUT).to(device)\n",
        "\n",
        "pos_weight = float((y == 0).sum() / max(1, (y == 1).sum()))\n",
        "weights = torch.tensor([1.0, pos_weight], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "-VMnJi0EFAL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Evaluate"
      ],
      "metadata": {
        "id": "nIn7ZWc_FBhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(split_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor, A_norm)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        y_true = y_tensor[split_mask].detach().cpu().numpy()\n",
        "        y_pred = preds[split_mask].detach().cpu().numpy()\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "        return acc, prec, rec, f1\n",
        "\n",
        "best_val = (-1, -1, -1, -1)\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X_tensor, A_norm)\n",
        "    loss = criterion(logits[train_mask_t], y_tensor[train_mask_t])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        tr = evaluate(train_mask_t)\n",
        "        va = evaluate(val_mask_t)\n",
        "        if va[3] > best_val[3]:\n",
        "            best_val = va\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Loss {loss.item():.4f} | \"\n",
        "              f\"Train A/P/R/F1: {tr[0]:.3f}/{tr[1]:.3f}/{tr[2]:.3f}/{tr[3]:.3f} | \"\n",
        "              f\"Val A/P/R/F1: {va[0]:.3f}/{va[1]:.3f}/{va[2]:.3f}/{va[3]:.3f}\")\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
        "\n",
        "ta = evaluate(test_mask_t)\n",
        "print(\"\\n=== TEST METRICS (Best F1 on Val) ===\")\n",
        "print(f\"Accuracy : {ta[0]:.4f}\")\n",
        "print(f\"Precision: {ta[1]:.4f}\")\n",
        "print(f\"Recall   : {ta[2]:.4f}\")\n",
        "print(f\"F1-score : {ta[3]:.4f}\")"
      ],
      "metadata": {
        "id": "3WafpvWQFC0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize graph"
      ],
      "metadata": {
        "id": "4TUFq4eqFD0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    classification_report, roc_curve, auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _mask_np(mask_t):\n",
        "    return mask_t.detach().cpu().numpy().astype(bool)\n",
        "\n",
        "def get_split_preds(mask_t):\n",
        "    mask = _mask_np(mask_t)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_tensor, A_norm)\n",
        "        probs1 = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "        preds = (probs1 >= 0.5).astype(int)\n",
        "    y_true = y_tensor.detach().cpu().numpy()[mask]\n",
        "    y_pred = preds[mask]\n",
        "    y_score = probs1[mask]\n",
        "    return y_true, y_pred, y_score\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, y_pred, _ = get_split_preds(m)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
        "    disp.plot(values_format=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{split_name} Confusion Matrix\")\n",
        "    plt.show()\n",
        "    print(f\"{split_name} classification report:\\n\",\n",
        "          classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "qgZ2KAHJFHXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, _, y_score = get_split_preds(m)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    axes[0].plot(fpr, tpr, label=f\"{split_name} AUC={auc(fpr,tpr):.3f}\")\n",
        "axes[0].plot([0,1],[0,1],'--',linewidth=1)\n",
        "axes[0].set_title(\"ROC Curve\"); axes[0].set_xlabel(\"FPR\"); axes[0].set_ylabel(\"TPR\"); axes[0].legend()\n",
        "\n",
        "for split_name, m in [(\"Train\", train_mask_t), (\"Val\", val_mask_t), (\"Test\", test_mask_t)]:\n",
        "    y_true, _, y_score = get_split_preds(m)\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
        "    axes[1].plot(rec, prec, label=f\"{split_name}\")\n",
        "axes[1].set_title(\"Precisionâ€“Recall Curve\"); axes[1].set_xlabel(\"Recall\"); axes[1].set_ylabel(\"Precision\"); axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZYwMzSw7FMk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = X_tensor.shape[0]\n",
        "deg_out = np.zeros(n, dtype=np.float32)\n",
        "for i, w in zip(rows, weights):\n",
        "    deg_out[i] += w\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(deg_out, bins=30)\n",
        "plt.title(\"Weighted Out-Degree Distribution\")\n",
        "plt.xlabel(\"Sum of edge weights (per node)\"); plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Quh3LijvFOUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOO CHEN KANG (TP065578)"
      ],
      "metadata": {
        "id": "meABj23S79Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lightgbm Model Code"
      ],
      "metadata": {
        "id": "kVt7y-5IHNTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib, json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# ------------------- Paths -------------------\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/lightgbm_top32.pkl\"\n",
        "FEATURE_PATH = \"/content/drive/MyDrive/Colab Notebooks/lightgbm_features_top32.json\"\n",
        "\n",
        "# ------------------- Load Data -------------------\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/phishing_mendeley_cleaned.csv\")\n",
        "\n",
        "# ------------------- Features & Target -------------------\n",
        "target_column = 'CLASS_LABEL'\n",
        "X = df.drop(columns=[target_column])\n",
        "y = df[target_column]\n",
        "\n",
        "# ------------------- Train/Test Split -------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ------------------- Best Parameters -------------------\n",
        "best_params = {\n",
        "    \"n_estimators\": 800,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 127,\n",
        "    \"feature_fraction\": 0.7,\n",
        "    \"bagging_fraction\": 0.6,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_child_samples\": 20,\n",
        "    \"reg_alpha\": 0,\n",
        "    \"reg_lambda\": 0,\n",
        "    \"random_state\": 42,\n",
        "    \"n_jobs\": -1\n",
        "}\n",
        "\n",
        "# ------------------- Train Model -------------------\n",
        "lgbm = LGBMClassifier(**best_params)\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "# ------------------- Evaluate -------------------\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"Precision\": precision_score(y_test, y_pred),\n",
        "    \"Recall\": recall_score(y_test, y_pred),\n",
        "    \"F1 Score\": f1_score(y_test, y_pred),\n",
        "    \"ROC-AUC\": float(roc_auc_score(y_test, y_pred))\n",
        "}\n",
        "\n",
        "# ------------------- Feature Importance -------------------\n",
        "feat_imp = pd.Series(lgbm.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "top32_features = feat_imp.head(32)\n",
        "print(\"\\nTop-32 Features:\\n\", top32_features)\n",
        "\n",
        "# ------------------- Save Model & Features -------------------\n",
        "joblib.dump(lgbm, MODEL_PATH)\n",
        "json.dump(list(top32_features.index), open(FEATURE_PATH, \"w\"))\n",
        "\n",
        "print(f\"\\nSaved model -> {MODEL_PATH}\")\n",
        "print(f\"Saved features -> {FEATURE_PATH}\")\n",
        "\n",
        "# Convert to DataFrame for nice table view\n",
        "metrics_df = pd.DataFrame(metrics, index=[\"LightGBM (Top-32 Features)\"])\n",
        "print(\"\\n=== Final Top-32 Model Performance ===\")\n",
        "print(metrics_df)\n",
        "\n",
        "# ------------------- Confusion Matrix -------------------\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Legitimate\", \"Phishing\"],\n",
        "            yticklabels=[\"Legitimate\", \"Phishing\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "agQoPLyZHOsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENG YI LING (TP065686)"
      ],
      "metadata": {
        "id": "x_b5p5YN7-NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============ FINAL RESULT (XGBOOST) ============"
      ],
      "metadata": {
        "id": "sXaTWech89Le"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Ife0CmG9Ff5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}